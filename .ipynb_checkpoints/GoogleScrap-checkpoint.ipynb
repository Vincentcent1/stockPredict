{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archived\n",
    "# MMDDYYYY\n",
    "# start = ['10','10','2016']\n",
    "# end = ['11','10','2016']\n",
    "# date = 'tbs=cdr%3A1%2Ccd_min%3A'+ start[0] + '%2F' + start[1] +'%2F' + start[2] + '%2Ccd_max%3A' + \\\n",
    "#         end[0] + '%2F' + end[1] + '%2F' + end[2]\n",
    "# ticker = '\"GOOGLE\"'\n",
    "# webfilter = \"Reuters.com\"\n",
    "# website ='https://www.google.com/search?&' + date + '&tbm=nws&q=site:{}+'.format(webfilter) + ticker\n",
    "# browser = webdriver.Chrome()\n",
    "# browser.get(website)\n",
    "# html = browser.page_source\n",
    "# # browser.quit()\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# # a = soup.find('section', 'wrapper')\n",
    "# websites = []\n",
    "# for link in soup.findAll('a', attrs={'href': re.compile(\"(^https://)(?!.*google.com)(?!.*youtube.com)(?!.*blogger.com)\")}):\n",
    "#     link_str = link.get('href')\n",
    "#     if link_str not in websites:\n",
    "#         websites.append(link_str)\n",
    "# # for i in websites:\n",
    "# #     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Montly Fool\n",
    "# for site in websites:\n",
    "#     print(\"================================\")\n",
    "    \n",
    "#     sitetext = ''\n",
    "#     r = requests.get(site)\n",
    "#     websoup = BeautifulSoup(r.text)\n",
    "#     author_box = websoup.find('div',attrs={'class': re.compile(\"author-name\")})\n",
    "#     print(\"{}\".format(websoup.title.string))\n",
    "#     print()\n",
    "#     if (author_box):\n",
    "#         for e in author_box:\n",
    "#             if (e):\n",
    "#                 print(\"Author: {}\".format(e.strip()))\n",
    "#                 print()\n",
    "#             else:\n",
    "#                 print(\"Author: {}\".format(\"None\"))\n",
    "#                 print() \n",
    "#     else:\n",
    "#         print(\"Author: {}\".format(\"None\"))\n",
    "#         print()\n",
    "#     extractedweb = websoup.findAll('p')\n",
    "    \n",
    "#     for p in extractedweb:\n",
    "#         if(p.find(text=True)):\n",
    "#             if p.em:\n",
    "#                 continue\n",
    "#             elif (p.attrs=={'class':['primary_author-bio']}):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sitetext += p.text.strip() + \" \"\n",
    "#     print(sitetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MarketWatch\n",
    "# for site in websites:\n",
    "#     print(\"================================\")\n",
    "    \n",
    "#     sitetext = ''\n",
    "# #     browser.get(site)\n",
    "# #     html = browser.page_source\n",
    "# #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "#     r = requests.get(site)\n",
    "#     websoup = BeautifulSoup(r.text)\n",
    "#     author_box = websoup.find('div',attrs={'class': re.compile(\"byline\")})\n",
    "#     print(\"{}\".format(websoup.title.string))\n",
    "#     print()\n",
    "#     if (author_box):\n",
    "#         for e in author_box:\n",
    "#             if (e.string.strip() == \"By\" or e.string.strip() == \"\"):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 if (e):\n",
    "#                     print(\"Author: {}\".format(e.string.strip()))\n",
    "#                     print()\n",
    "#                 else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print() \n",
    "#     else:\n",
    "#         print(\"Author: {}\".format(\"None\"))\n",
    "#         print()\n",
    "#     extractedweb = websoup.findAll('p')\n",
    "    \n",
    "#     for p in extractedweb:\n",
    "#         if(p.find(text=True)):\n",
    "#             if (p.attrs=={'class':['bio']} or p.attrs=={'class':['copyright']} or p.attrs=={'class':['text', 'text--terms']} or p.attrs=={'class': ['correction']}):\n",
    "#                 continue\n",
    "#             elif (p.text.strip() == \"Join the conversation\"):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sitetext += p.text.strip() + \" \"\n",
    "#     print(sitetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Investopedia\n",
    "# for site in websites:\n",
    "#     print(\"================================\")\n",
    "    \n",
    "#     sitetext = ''\n",
    "# #     browser.get(site)\n",
    "# #     html = browser.page_source\n",
    "# #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "#     r = requests.get(site)\n",
    "#     websoup = BeautifulSoup(r.text)\n",
    "#     author_box = websoup.find('span',attrs={'class': re.compile(\"by-author\")})\n",
    "#     print(\"{}\".format(websoup.title.string))\n",
    "#     print()\n",
    "#     if (author_box):\n",
    "#         counter = 0\n",
    "#         for e in author_box:\n",
    "#             if (counter == 1):\n",
    "#                 if (e):\n",
    "#                     print(\"Author: {}\".format(e.string.strip()))\n",
    "#                     print()\n",
    "#                 else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print() \n",
    "#             counter += 1\n",
    "#     else:\n",
    "#         print(\"Author: {}\".format(\"None\"))\n",
    "#         print()\n",
    "#     extractedweb = websoup.findAll('p')\n",
    "    \n",
    "#     for p in extractedweb:\n",
    "#         if(p.find(text=True)):\n",
    "#             if (p.attrs=={'class':['bio']} or p.attrs=={'class':['highlight']}):\n",
    "#                 continue\n",
    "#             elif (p.text.strip() == \"Want to learn how to invest?\"):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sitetext += p.text.strip()+ \" \"\n",
    "#     print(sitetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CNBC\n",
    "# for site in websites:\n",
    "#     print(\"================================\")\n",
    "    \n",
    "#     sitetext = ''\n",
    "# #     browser.get(site)\n",
    "# #     html = browser.page_source\n",
    "# #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "#     r = requests.get(site)\n",
    "#     websoup = BeautifulSoup(r.text)\n",
    "#     author_box = websoup.find('div',attrs={'itemprop': re.compile(\"author\")})\n",
    "#     print(\"{}\".format(websoup.title.string))\n",
    "#     print()\n",
    "#     if (author_box):\n",
    "#         counter = 0\n",
    "#         for e in author_box:\n",
    "#             if (counter == 1):\n",
    "#                 if (e):\n",
    "#                     print(\"Author: {}\".format(e.string.strip()))\n",
    "#                     print()\n",
    "#                 else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print() \n",
    "#             counter += 1\n",
    "#     else:\n",
    "#         print(\"Author: {}\".format(\"None\"))\n",
    "#         print()\n",
    "#     extractedweb = websoup.findAll('p')\n",
    "    \n",
    "#     for p in extractedweb:\n",
    "#         if(p.find(text=True)):\n",
    "#             if (p.attrs=={'class':['label_share']} or p.attrs=={'class':['label']} or p.attrs=={'class':['Footer-sectionInfo']} or p.attrs=={'class':['Footer-copyright']}):\n",
    "#                 continue\n",
    "#             elif (p.input or p.em):\n",
    "#                 continue\n",
    "#             elif (p.text.strip() == \"Want to learn how to invest?\"):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sitetext += p.text.strip()+ \" \"\n",
    "#     print(sitetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reuters\n",
    "# for site in websites:\n",
    "#     print(\"================================\")\n",
    "#     sitetext = ''\n",
    "#     r = requests.get(site)\n",
    "#     websoup = BeautifulSoup(r.text)\n",
    "\n",
    "#     author_box = websoup.find(\"meta\",attrs = {'name':'Author'}, content=True)\n",
    "#     print(author_box['content'])\n",
    "#     print(\"{}\".format(websoup.title.string))\n",
    "#     print()\n",
    "\n",
    "#     if author_box:\n",
    "#         for e in author_box:\n",
    "#             print(e)\n",
    "#             if e:\n",
    "#                 print(\"Author: {}\".format(e.string.strip()))\n",
    "#                 print()\n",
    "#             else:\n",
    "#                 print(\"Author: {}\".format(\"None\"))\n",
    "#                 print() \n",
    "#     else:\n",
    "#         print(\"Author: {}\".format(\"None\"))\n",
    "#         print()\n",
    "        \n",
    "#     extractedweb = websoup.findAll('p')\n",
    "#     for p in extractedweb:\n",
    "#         if(p.find(text=True)):\n",
    "#             #print(p.attrs)\n",
    "#             if p.em:\n",
    "#                 continue\n",
    "#             if p.attrs == {'class': ['BylineBar_reading-time'], 'style': 'color:undefined'}:\n",
    "#                 continue\n",
    "#             if p.attrs == {'class': ['Attribution_content']}:\n",
    "#                 continue\n",
    "#             if p.attrs == {'div':['Attribution_container']}:\n",
    "#                 continue\n",
    "#             if p.span:\n",
    "#                 continue\n",
    "#             elif (p.attrs=={'a'}):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sitetext += p.text.strip() + \" \"\n",
    "#     print(sitetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper(start_date, end_date, ticker):\n",
    "    '''\n",
    "    start_date: int value in form MMDDYYYY\n",
    "    end_date: int value in form MMDDYYYY\n",
    "    ticker: company ticker to search\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    scrapesites = ['reuters.com', 'investopedia.com', 'cnbc.com', 'fool.com', 'marketwatch.com']\n",
    "    \n",
    "    start = [str(int(int(start_date)/1000000)),str(int(int(start_date)/10000)%100),str(int(start_date)%10000)]\n",
    "    end = [str(int(int(end_date)/1000000)),str(int(int(end_date)/10000)%100),str(int(end_date)%10000)]\n",
    "    date = 'tbs=cdr%3A1%2Ccd_min%3A'+ start[0] + '%2F' + start[1] +'%2F' + start[2] + '%2Ccd_max%3A' + \\\n",
    "        end[0] + '%2F' + end[1] + '%2F' + end[2]\n",
    "    ticker = '\"' + ticker + '\"'\n",
    "    for ws in scrapesites:\n",
    "        authorval = \"\"\n",
    "        textval = \"\"\n",
    "        titleval = \"\"\n",
    "        website ='https://www.google.com/search?&' + date + '&tbm=nws&q=site:{}+'.format(ws) + ticker\n",
    "        browser = webdriver.Firefox()\n",
    "        browser.get(website)\n",
    "        html = browser.page_source\n",
    "        browser.quit()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # a = soup.find('section', 'wrapper')\n",
    "        websites = []\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(\"(^https://)(?!.*google.com)(?!.*youtube.com)(?!.*blogger.com)\")}):\n",
    "            link_str = link.get('href')\n",
    "            if link_str not in websites:\n",
    "                websites.append(link_str)\n",
    "        if(ws == 'reuters.com'):\n",
    "            #Reuters\n",
    "            for site in websites:\n",
    "#                 print(\"================================\")\n",
    "                sitetext = ''\n",
    "                r = requests.get(site)\n",
    "                websoup = BeautifulSoup(r.text)\n",
    "\n",
    "                author_box = websoup.find(\"meta\",attrs = {'name':'Author'}, content=True)\n",
    "#                 print(author_box['content'])\n",
    "#                 print(\"{}\".format(websoup.title.string))\n",
    "#                 print()\n",
    "                titleval = websoup.title.string\n",
    "                if author_box:\n",
    "                    for e in author_box:\n",
    "                        print(e)\n",
    "                        if e:\n",
    "#                             print(\"Author: {}\".format(e.string.strip()))\n",
    "#                             print()\n",
    "                            authorval = e.string.strip()\n",
    "                            \n",
    "                        else:\n",
    "#                             print(\"Author: {}\".format(\"None\"))\n",
    "#                             print() \n",
    "                            authorval = \" \"\n",
    "                else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print()\n",
    "                    authorval = \" \"\n",
    "                print(authorval)\n",
    "                extractedweb = websoup.findAll('p')\n",
    "                for p in extractedweb:\n",
    "                    if(p.find(text=True)):\n",
    "                        #print(p.attrs)\n",
    "                        if p.em:\n",
    "                            continue\n",
    "                        if p.attrs == {'class': ['BylineBar_reading-time'], 'style': 'color:undefined'}:\n",
    "                            continue\n",
    "                        if p.attrs == {'class': ['Attribution_content']}:\n",
    "                            continue\n",
    "                        if p.attrs == {'div':['Attribution_container']}:\n",
    "                            continue\n",
    "                        if p.span:\n",
    "                            continue\n",
    "                        elif (p.attrs=={'a'}):\n",
    "                            continue\n",
    "                        else:\n",
    "                            sitetext += p.text.strip() + \" \"\n",
    "#                 print(sitetext)\n",
    "                textval = sitetext\n",
    "                df = df.append({\"Site\": \"Reuters\", \"Author\": authorval, \"Title\": titleval, \"Text\": textval, \"StartDate\": start_date, \"EndDate\": end_date}, ignore_index=True)\n",
    "#                 print(df.head(20))\n",
    "        elif (ws == 'investopedia.com'):\n",
    "            # Investopedia\n",
    "            for site in websites:\n",
    "#                 print(\"================================\")\n",
    "\n",
    "                sitetext = ''\n",
    "            #     browser.get(site)\n",
    "            #     html = browser.page_source\n",
    "            #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "                r = requests.get(site)\n",
    "                websoup = BeautifulSoup(r.text)\n",
    "                author_box = websoup.find('span',attrs={'class': re.compile(\"by-author\")})\n",
    "#                 print(\"{}\".format(websoup.title.string))\n",
    "#                 print()\n",
    "                titleval = websoup.title.string\n",
    "                if (author_box):\n",
    "                    counter = 0\n",
    "                    for e in author_box:\n",
    "                        if (counter == 1):\n",
    "                            if (e):\n",
    "#                                 print(\"Author: {}\".format(e.string.strip()))\n",
    "#                                 print()\n",
    "                                authorval = e.string.strip()\n",
    "                            else:\n",
    "#                                 print(\"Author: {}\".format(\"None\"))\n",
    "#                                 print() \n",
    "                                authorval = \" \"\n",
    "                        counter += 1\n",
    "                else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print()\n",
    "                    authorval = \" \"\n",
    "                extractedweb = websoup.findAll('p')\n",
    "\n",
    "                for p in extractedweb:\n",
    "                    if(p.find(text=True)):\n",
    "                        if (p.attrs=={'class':['bio']} or p.attrs=={'class':['highlight']}):\n",
    "                            continue\n",
    "                        elif (p.text.strip() == \"Want to learn how to invest?\"):\n",
    "                            continue\n",
    "                        else:\n",
    "                            sitetext += p.text.strip()+ \" \"\n",
    "                textval = sitetext\n",
    "                df = df.append({\"Site\": \"Investopedia\", \"Author\": authorval, \"Title\": titleval, \"Text\": textval, \"StartDate\": start_date, \"EndDate\": end_date}, ignore_index=True)\n",
    "#                 print(df.head(20))\n",
    "        elif (ws == 'cnbc.com'):\n",
    "            # CNBC\n",
    "            for site in websites:\n",
    "#                 print(\"================================\")\n",
    "\n",
    "                sitetext = ''\n",
    "            #     browser.get(site)\n",
    "            #     html = browser.page_source\n",
    "            #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "                r = requests.get(site)\n",
    "                websoup = BeautifulSoup(r.text)\n",
    "                author_box = websoup.find('div',attrs={'itemprop': re.compile(\"author\")})\n",
    "#                 print(\"{}\".format(websoup.title.string))\n",
    "#                 print()\n",
    "                titleval = websoup.title.string\n",
    "                if (author_box):\n",
    "                    counter = 0\n",
    "                    for e in author_box:\n",
    "                        if (counter == 1):\n",
    "                            if (e):\n",
    "#                                 print(\"Author: {}\".format(e.string.strip()))\n",
    "#                                 print()\n",
    "                                authorval = e.string.strip()\n",
    "                            else:\n",
    "#                                 print(\"Author: {}\".format(\"None\"))\n",
    "#                                 print() \n",
    "                                authorval = \" \"\n",
    "                        counter += 1\n",
    "                else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print()\n",
    "                    authorval = \" \"\n",
    "                extractedweb = websoup.findAll('p')\n",
    "\n",
    "                for p in extractedweb:\n",
    "                    if(p.find(text=True)):\n",
    "                        if (p.attrs=={'class':['label_share']} or p.attrs=={'class':['label']} or p.attrs=={'class':['Footer-sectionInfo']} or p.attrs=={'class':['Footer-copyright']}):\n",
    "                            continue\n",
    "                        elif (p.input or p.em):\n",
    "                            continue\n",
    "                        elif (p.text.strip() == \"Want to learn how to invest?\"):\n",
    "                            continue\n",
    "                        else:\n",
    "                            sitetext += p.text.strip()+ \" \"\n",
    "                textval = sitetext\n",
    "                df = df.append({\"Site\": \"CNBC\",\"Author\": authorval, \"Title\": titleval, \"Text\": textval, \"StartDate\": start_date, \"EndDate\": end_date}, ignore_index=True)\n",
    "#                 print(df.head(20))  \n",
    "        elif(ws == 'fool.com'):\n",
    "            # Montly Fool\n",
    "            for site in websites:\n",
    "#                 print(\"================================\")\n",
    "\n",
    "                sitetext = ''\n",
    "                r = requests.get(site)\n",
    "                websoup = BeautifulSoup(r.text)\n",
    "                author_box = websoup.find('div',attrs={'class': re.compile(\"author-name\")})\n",
    "#                 print(\"{}\".format(websoup.title.string))\n",
    "#                 print()\n",
    "                titleval = websoup.title.string\n",
    "                if (author_box):\n",
    "                    for e in author_box:\n",
    "                        if (e):\n",
    "#                             print(\"Author: {}\".format(e.strip()))\n",
    "#                             print()\n",
    "                            authorval = e.strip()\n",
    "                        else:\n",
    "#                             print(\"Author: {}\".format(\"None\"))\n",
    "#                             print() \n",
    "                            authorval = \" \"\n",
    "                else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print()\n",
    "                    authorval = \" \"\n",
    "                extractedweb = websoup.findAll('p')\n",
    "\n",
    "                for p in extractedweb:\n",
    "                    if(p.find(text=True)):\n",
    "                        if p.em:\n",
    "                            continue\n",
    "                        elif (p.attrs=={'class':['primary_author-bio']}):\n",
    "                            continue\n",
    "                        else:\n",
    "                            sitetext += p.text.strip() + \" \"\n",
    "                textval = sitetext\n",
    "                df = df.append({\"Site\": \"Fool\",\"Author\": authorval, \"Title\": titleval, \"Text\": textval, \"StartDate\": start_date, \"EndDate\": end_date}, ignore_index=True)\n",
    "#                 print(df.head(20))\n",
    "        elif (ws == 'marketwatch.com'):\n",
    "            # MarketWatch\n",
    "            for site in websites:\n",
    "#                 print(\"================================\")\n",
    "\n",
    "                sitetext = ''\n",
    "            #     browser.get(site)\n",
    "            #     html = browser.page_source\n",
    "            #     websoup = BeautifulSoup(html, 'html.parser')\n",
    "                r = requests.get(site)\n",
    "                websoup = BeautifulSoup(r.text)\n",
    "                author_box = websoup.find('div',attrs={'class': re.compile(\"byline\")})\n",
    "#                 print(\"{}\".format(websoup.title.string))\n",
    "#                 print()\n",
    "                titleval = websoup.title.string\n",
    "                if (author_box):\n",
    "                    for e in author_box:\n",
    "                        if (e.string.strip() == \"By\" or e.string.strip() == \"\"):\n",
    "                            continue\n",
    "                        else:\n",
    "                            if (e):\n",
    "#                                 print(\"Author: {}\".format(e.string.strip()))\n",
    "#                                 print()\n",
    "                                authorval = e.string.strip()\n",
    "                            else:\n",
    "#                                 print(\"Author: {}\".format(\"None\"))\n",
    "#                                 print() \n",
    "                                authorval = \" \"\n",
    "                else:\n",
    "#                     print(\"Author: {}\".format(\"None\"))\n",
    "#                     print()\n",
    "                    authorval = \" \"\n",
    "                extractedweb = websoup.findAll('p')\n",
    "\n",
    "                for p in extractedweb:\n",
    "                    if(p.find(text=True)):\n",
    "                        if (p.attrs=={'class':['bio']} or p.attrs=={'class':['copyright']} or p.attrs=={'class':['text', 'text--terms']} or p.attrs=={'class': ['correction']}):\n",
    "                            continue\n",
    "                        elif (p.text.strip() == \"Join the conversation\"):\n",
    "                            continue\n",
    "                        else:\n",
    "                            sitetext += p.text.strip() + \" \"\n",
    "                textval = sitetext\n",
    "                df = df.append({\"Site\": \"MarketWatch\", \"Author\": authorval, \"Title\": titleval, \"Text\": textval, \"StartDate\": start_date, \"EndDate\": end_date}, ignore_index=True)\n",
    "#                 print(df.head(20))\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "\n",
      "  Author   EndDate     Site StartDate  \\\n",
      "0         12082016  Reuters  12012016   \n",
      "\n",
      "                                                Text  \\\n",
      "0  (Reuters) - Alphabet’s Google is on track to p...   \n",
      "\n",
      "                                               Title  \n",
      "0  \n",
      "                Google meets renewable energy...  \n",
      "================================\n",
      "\n",
      "  Author   EndDate     Site StartDate  \\\n",
      "0         12082016  Reuters  12012016   \n",
      "1         12082016  Reuters  12012016   \n",
      "\n",
      "                                                Text  \\\n",
      "0  (Reuters) - Alphabet’s Google is on track to p...   \n",
      "1  BRUSSELS (Reuters) - U.S. tech giants includin...   \n",
      "\n",
      "                                               Title  \n",
      "0  \n",
      "                Google meets renewable energy...  \n",
      "1  \n",
      "                EU urges U.S. tech giants to ...  \n",
      "================================\n",
      "\n",
      "  Author   EndDate     Site StartDate  \\\n",
      "0         12082016  Reuters  12012016   \n",
      "1         12082016  Reuters  12012016   \n",
      "2         12082016  Reuters  12012016   \n",
      "\n",
      "                                                Text  \\\n",
      "0  (Reuters) - Alphabet’s Google is on track to p...   \n",
      "1  BRUSSELS (Reuters) - U.S. tech giants includin...   \n",
      "2  FRANKFURT/HELSINKI (Reuters) - Nokia smartphon...   \n",
      "\n",
      "                                               Title  \n",
      "0  \n",
      "                Google meets renewable energy...  \n",
      "1  \n",
      "                EU urges U.S. tech giants to ...  \n",
      "2  \n",
      "                Fallen smartphone brand Nokia...  \n",
      "================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-eadfb2126752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewEndDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mscrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewStartDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewEndDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GOOGLE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdateStart\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdateEnd\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-55f03de82c69>\u001b[0m in \u001b[0;36mscrapper\u001b[0;34m(start_date, end_date, ticker)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0msitetext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mwebsoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;34m\"\"\"Closes all adapters and as such the session\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mwhich\u001b[0m \u001b[0mcloses\u001b[0m \u001b[0many\u001b[0m \u001b[0mpooled\u001b[0m \u001b[0mconnections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \"\"\"\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoolmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproxy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mused\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnection_from_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/urllib3/_collections.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_pool_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection_pool_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         self.pools = RecentlyUsedContainer(num_pools,\n\u001b[0;32m--> 156\u001b[0;31m                                            dispose_func=lambda p: p.close())\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# Locally set the pool classes and keys so other PoolManagers can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stockPredict/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m                     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dateStart = dt.datetime(2016,12,1)\n",
    "dateEnd = dt.datetime(2016,12,8)\n",
    "newStartDate = dateStart.strftime(\"%m%d%Y\")\n",
    "newEndDate = dateEnd.strftime(\"%m%d%Y\")\n",
    "# date_start = dt.strftime(startstart,\"%d%m%Y\")\n",
    "# print(datestart)\n",
    "# print(newStartDate)\n",
    "# print(newEndDate)\n",
    "companies_to_run = ['GOOGLE','APPLE','AMAZON','ALIBABA','BANKOFAMERICA','Anheuser-BuschInBev','CITIGROUP',\n",
    "                   'CHINAMOBILE','CHEVRONCORPORATION','FACEBOOK','HOMEDEPOTINC', 'INTELCOPORATION', 'JOHNSON&JOHNSON',\n",
    "                   'JPMorganChase','MICROSOFT','Novartis', 'ORACLECORPORATION', 'Pfizer', 'Procter&Gamble',\n",
    "                   'RoyalDutchShellClassA', 'RoyalDutchShellClassB', 'AT&T', 'UnitedHealthGroup', 'VisaInc',\n",
    "                   'VerizonCommunications', 'Wells Fargo', 'Walmart', 'ExxonMobilCorporation']\n",
    "counter = 0\n",
    "masterlist = []\n",
    "\n",
    "for companies in companies_to_run:\n",
    "    while (int(newEndDate)%10000 != 2018):\n",
    "        print(newEndDate)\n",
    "        extracted = scrapper(newStartDate, newEndDate, companies)\n",
    "        masterlist.append(extracted)\n",
    "        counter += 1\n",
    "        if (counter %10 == 0):\n",
    "            output = pd.concat(masterlist)\n",
    "            output.to_csv(\"{}{}.csv\".format(companies,counter), sep=\",\", index=False)\n",
    "            masterlist = []\n",
    "\n",
    "        dateStart += dt.timedelta(days = 7)\n",
    "        dateEnd += dt.timedelta(days = 7)\n",
    "        newStartDate = dateStart.strftime(\"%m%d%Y\")\n",
    "        newEndDate = dateEnd.strftime(\"%m%d%Y\")\n",
    "\n",
    "    output = pd.concat(masterlist)\n",
    "    output.to_csv(\"{}{}.csv\".format(companies,counter), sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
