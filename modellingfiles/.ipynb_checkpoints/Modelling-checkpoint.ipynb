{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"finaldata.csv\", index_col = False)\n",
    "df = df.drop([\"FractionalIncrease\", \"High\", \"Low\", \"Positive\", \"Open\"],axis=1)\n",
    "df = df.fillna(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop([\"Delta\"], axis=1), df[\"Delta\"], test_size=0.20, random_state=4)\n",
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>ReuterCredit</th>\n",
       "      <th>reuter_score</th>\n",
       "      <th>InvestCredit</th>\n",
       "      <th>invest_score</th>\n",
       "      <th>FoolCredit</th>\n",
       "      <th>fool_score</th>\n",
       "      <th>CnbcCredit</th>\n",
       "      <th>cnbc_score</th>\n",
       "      <th>MarketCredit</th>\n",
       "      <th>market_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.060</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>0.497530</td>\n",
       "      <td>0.976303</td>\n",
       "      <td>0.911567</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>0.984530</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>0.924140</td>\n",
       "      <td>0.977183</td>\n",
       "      <td>0.895209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.400</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>0.577730</td>\n",
       "      <td>0.931008</td>\n",
       "      <td>0.873125</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>0.992450</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>0.745560</td>\n",
       "      <td>0.953855</td>\n",
       "      <td>0.488955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.410</td>\n",
       "      <td>0.945488</td>\n",
       "      <td>0.572960</td>\n",
       "      <td>0.937707</td>\n",
       "      <td>0.959300</td>\n",
       "      <td>0.945488</td>\n",
       "      <td>0.990220</td>\n",
       "      <td>0.945488</td>\n",
       "      <td>0.942220</td>\n",
       "      <td>0.947621</td>\n",
       "      <td>0.421736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.260</td>\n",
       "      <td>0.958241</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.950460</td>\n",
       "      <td>0.982900</td>\n",
       "      <td>0.952755</td>\n",
       "      <td>0.983064</td>\n",
       "      <td>0.957221</td>\n",
       "      <td>0.957580</td>\n",
       "      <td>0.958288</td>\n",
       "      <td>0.694864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.770</td>\n",
       "      <td>0.968660</td>\n",
       "      <td>0.647170</td>\n",
       "      <td>0.957466</td>\n",
       "      <td>0.952050</td>\n",
       "      <td>0.961451</td>\n",
       "      <td>0.977010</td>\n",
       "      <td>0.968150</td>\n",
       "      <td>0.985720</td>\n",
       "      <td>0.968683</td>\n",
       "      <td>0.970160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.170</td>\n",
       "      <td>0.971340</td>\n",
       "      <td>0.520433</td>\n",
       "      <td>0.961143</td>\n",
       "      <td>0.963560</td>\n",
       "      <td>0.969940</td>\n",
       "      <td>0.979464</td>\n",
       "      <td>0.973290</td>\n",
       "      <td>0.970120</td>\n",
       "      <td>0.973557</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.880</td>\n",
       "      <td>0.951983</td>\n",
       "      <td>0.613570</td>\n",
       "      <td>0.950226</td>\n",
       "      <td>0.875420</td>\n",
       "      <td>0.951283</td>\n",
       "      <td>0.983930</td>\n",
       "      <td>0.952958</td>\n",
       "      <td>0.728020</td>\n",
       "      <td>0.954236</td>\n",
       "      <td>0.736736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.958883</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.957756</td>\n",
       "      <td>0.981425</td>\n",
       "      <td>0.958191</td>\n",
       "      <td>0.976930</td>\n",
       "      <td>0.959028</td>\n",
       "      <td>0.851680</td>\n",
       "      <td>0.960240</td>\n",
       "      <td>0.635125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.050</td>\n",
       "      <td>0.955338</td>\n",
       "      <td>0.468512</td>\n",
       "      <td>0.949628</td>\n",
       "      <td>0.968350</td>\n",
       "      <td>0.954649</td>\n",
       "      <td>0.983340</td>\n",
       "      <td>0.955067</td>\n",
       "      <td>0.317280</td>\n",
       "      <td>0.956246</td>\n",
       "      <td>0.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-44.050</td>\n",
       "      <td>0.945884</td>\n",
       "      <td>0.447844</td>\n",
       "      <td>0.946424</td>\n",
       "      <td>0.988067</td>\n",
       "      <td>0.945540</td>\n",
       "      <td>0.936760</td>\n",
       "      <td>0.945749</td>\n",
       "      <td>0.508890</td>\n",
       "      <td>0.946911</td>\n",
       "      <td>0.424650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.730</td>\n",
       "      <td>0.955978</td>\n",
       "      <td>0.587855</td>\n",
       "      <td>0.956641</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>0.956920</td>\n",
       "      <td>0.903220</td>\n",
       "      <td>0.956014</td>\n",
       "      <td>0.935950</td>\n",
       "      <td>0.954971</td>\n",
       "      <td>0.751375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.770</td>\n",
       "      <td>0.964131</td>\n",
       "      <td>0.563689</td>\n",
       "      <td>0.963630</td>\n",
       "      <td>0.921250</td>\n",
       "      <td>0.963369</td>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.964145</td>\n",
       "      <td>0.720800</td>\n",
       "      <td>0.962853</td>\n",
       "      <td>0.937718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.580</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.115389</td>\n",
       "      <td>0.948204</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.948073</td>\n",
       "      <td>0.762780</td>\n",
       "      <td>0.948461</td>\n",
       "      <td>0.685980</td>\n",
       "      <td>0.948273</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.230</td>\n",
       "      <td>0.958082</td>\n",
       "      <td>0.881422</td>\n",
       "      <td>0.952601</td>\n",
       "      <td>0.604780</td>\n",
       "      <td>0.957892</td>\n",
       "      <td>0.991100</td>\n",
       "      <td>0.958086</td>\n",
       "      <td>0.922800</td>\n",
       "      <td>0.958450</td>\n",
       "      <td>0.909411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.620</td>\n",
       "      <td>0.965245</td>\n",
       "      <td>0.288322</td>\n",
       "      <td>0.962504</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>0.965150</td>\n",
       "      <td>0.995150</td>\n",
       "      <td>0.965247</td>\n",
       "      <td>0.862680</td>\n",
       "      <td>0.965887</td>\n",
       "      <td>0.896427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.840</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.959476</td>\n",
       "      <td>0.993720</td>\n",
       "      <td>0.964096</td>\n",
       "      <td>0.994780</td>\n",
       "      <td>0.962638</td>\n",
       "      <td>0.658450</td>\n",
       "      <td>0.964465</td>\n",
       "      <td>0.510431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-29.140</td>\n",
       "      <td>0.953324</td>\n",
       "      <td>0.579109</td>\n",
       "      <td>0.951744</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.954054</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.953325</td>\n",
       "      <td>0.746890</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.651292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.590</td>\n",
       "      <td>0.964751</td>\n",
       "      <td>0.707411</td>\n",
       "      <td>0.963961</td>\n",
       "      <td>0.741020</td>\n",
       "      <td>0.963285</td>\n",
       "      <td>0.993320</td>\n",
       "      <td>0.964752</td>\n",
       "      <td>0.724320</td>\n",
       "      <td>0.965208</td>\n",
       "      <td>0.370440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2.480</td>\n",
       "      <td>0.972124</td>\n",
       "      <td>0.529630</td>\n",
       "      <td>0.968728</td>\n",
       "      <td>0.761230</td>\n",
       "      <td>0.971391</td>\n",
       "      <td>0.994190</td>\n",
       "      <td>0.972124</td>\n",
       "      <td>0.696450</td>\n",
       "      <td>0.972353</td>\n",
       "      <td>0.334078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-8.460</td>\n",
       "      <td>0.960749</td>\n",
       "      <td>0.080812</td>\n",
       "      <td>0.959436</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.960767</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.961134</td>\n",
       "      <td>0.968690</td>\n",
       "      <td>0.960863</td>\n",
       "      <td>0.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.700</td>\n",
       "      <td>0.960695</td>\n",
       "      <td>-0.053078</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.957933</td>\n",
       "      <td>0.880200</td>\n",
       "      <td>0.961080</td>\n",
       "      <td>0.698273</td>\n",
       "      <td>0.957981</td>\n",
       "      <td>0.671618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30.260</td>\n",
       "      <td>0.955544</td>\n",
       "      <td>0.386140</td>\n",
       "      <td>0.957643</td>\n",
       "      <td>0.908040</td>\n",
       "      <td>0.954011</td>\n",
       "      <td>0.969660</td>\n",
       "      <td>0.957065</td>\n",
       "      <td>0.650780</td>\n",
       "      <td>0.953938</td>\n",
       "      <td>0.401945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>60.290</td>\n",
       "      <td>0.949518</td>\n",
       "      <td>0.312682</td>\n",
       "      <td>0.950759</td>\n",
       "      <td>0.954340</td>\n",
       "      <td>0.948030</td>\n",
       "      <td>0.966540</td>\n",
       "      <td>0.951038</td>\n",
       "      <td>0.586990</td>\n",
       "      <td>0.947897</td>\n",
       "      <td>0.677808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.962977</td>\n",
       "      <td>0.226213</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.536350</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.986918</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.946600</td>\n",
       "      <td>0.951843</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-8.090</td>\n",
       "      <td>0.951986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963993</td>\n",
       "      <td>0.972050</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.949935</td>\n",
       "      <td>0.583620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>35.800</td>\n",
       "      <td>0.955422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961097</td>\n",
       "      <td>0.981330</td>\n",
       "      <td>0.955229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952130</td>\n",
       "      <td>0.624050</td>\n",
       "      <td>0.953573</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.960</td>\n",
       "      <td>0.953823</td>\n",
       "      <td>0.061150</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.977680</td>\n",
       "      <td>0.948473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.947794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950412</td>\n",
       "      <td>0.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.270</td>\n",
       "      <td>0.965274</td>\n",
       "      <td>0.619411</td>\n",
       "      <td>0.969990</td>\n",
       "      <td>0.952140</td>\n",
       "      <td>0.961259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964578</td>\n",
       "      <td>0.888620</td>\n",
       "      <td>0.964585</td>\n",
       "      <td>0.960550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-56.210</td>\n",
       "      <td>0.962877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950795</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.951528</td>\n",
       "      <td>0.418660</td>\n",
       "      <td>0.963225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28.850</td>\n",
       "      <td>0.950824</td>\n",
       "      <td>0.315725</td>\n",
       "      <td>0.955866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.943408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.943841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950969</td>\n",
       "      <td>0.765990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.960695</td>\n",
       "      <td>0.424633</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.985750</td>\n",
       "      <td>0.957933</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>0.961080</td>\n",
       "      <td>0.484925</td>\n",
       "      <td>0.957981</td>\n",
       "      <td>0.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.782</td>\n",
       "      <td>0.955544</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.957643</td>\n",
       "      <td>0.973533</td>\n",
       "      <td>0.954011</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.957065</td>\n",
       "      <td>0.627280</td>\n",
       "      <td>0.953938</td>\n",
       "      <td>0.979720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.094</td>\n",
       "      <td>0.949518</td>\n",
       "      <td>0.451640</td>\n",
       "      <td>0.950759</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.948030</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.951038</td>\n",
       "      <td>0.602560</td>\n",
       "      <td>0.947897</td>\n",
       "      <td>0.661886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.962977</td>\n",
       "      <td>0.700250</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.987650</td>\n",
       "      <td>0.962233</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.711222</td>\n",
       "      <td>0.951843</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-1.942</td>\n",
       "      <td>0.951986</td>\n",
       "      <td>0.639822</td>\n",
       "      <td>0.963993</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.981525</td>\n",
       "      <td>0.952366</td>\n",
       "      <td>0.864486</td>\n",
       "      <td>0.949935</td>\n",
       "      <td>0.994050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.955422</td>\n",
       "      <td>0.811286</td>\n",
       "      <td>0.961097</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.955229</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.952130</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.953573</td>\n",
       "      <td>0.747086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>-1.696</td>\n",
       "      <td>0.953823</td>\n",
       "      <td>0.294720</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.989000</td>\n",
       "      <td>0.948473</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.947794</td>\n",
       "      <td>0.859586</td>\n",
       "      <td>0.950412</td>\n",
       "      <td>0.988320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.464</td>\n",
       "      <td>0.965274</td>\n",
       "      <td>0.583217</td>\n",
       "      <td>0.969990</td>\n",
       "      <td>0.993200</td>\n",
       "      <td>0.961259</td>\n",
       "      <td>0.864871</td>\n",
       "      <td>0.964578</td>\n",
       "      <td>0.718260</td>\n",
       "      <td>0.964585</td>\n",
       "      <td>0.931250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2.011</td>\n",
       "      <td>0.962877</td>\n",
       "      <td>0.375360</td>\n",
       "      <td>0.968284</td>\n",
       "      <td>0.983233</td>\n",
       "      <td>0.950795</td>\n",
       "      <td>0.792410</td>\n",
       "      <td>0.951528</td>\n",
       "      <td>0.200440</td>\n",
       "      <td>0.963225</td>\n",
       "      <td>0.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-1.223</td>\n",
       "      <td>0.950824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.955866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.943408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.943841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2.811</td>\n",
       "      <td>0.958651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.958192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.324</td>\n",
       "      <td>0.959980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.958124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.956828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959751</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.961173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.961302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.946764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.949788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.943445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946366</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.948680</td>\n",
       "      <td>0.386709</td>\n",
       "      <td>0.951733</td>\n",
       "      <td>0.977900</td>\n",
       "      <td>0.948636</td>\n",
       "      <td>0.727020</td>\n",
       "      <td>0.944258</td>\n",
       "      <td>0.274880</td>\n",
       "      <td>0.948479</td>\n",
       "      <td>0.561629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-1.102</td>\n",
       "      <td>0.962394</td>\n",
       "      <td>0.743711</td>\n",
       "      <td>0.958318</td>\n",
       "      <td>0.990800</td>\n",
       "      <td>0.964341</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.957967</td>\n",
       "      <td>0.377650</td>\n",
       "      <td>0.962002</td>\n",
       "      <td>0.872740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.079</td>\n",
       "      <td>0.961949</td>\n",
       "      <td>0.412786</td>\n",
       "      <td>0.956678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957371</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.970783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-1.182</td>\n",
       "      <td>0.968381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957067</td>\n",
       "      <td>0.897750</td>\n",
       "      <td>0.966314</td>\n",
       "      <td>0.926580</td>\n",
       "      <td>0.964720</td>\n",
       "      <td>0.251460</td>\n",
       "      <td>0.963391</td>\n",
       "      <td>0.127437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-1.052</td>\n",
       "      <td>0.966957</td>\n",
       "      <td>0.635550</td>\n",
       "      <td>0.959131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968095</td>\n",
       "      <td>0.853040</td>\n",
       "      <td>0.965569</td>\n",
       "      <td>0.462050</td>\n",
       "      <td>0.964240</td>\n",
       "      <td>0.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1.588</td>\n",
       "      <td>0.973950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.973446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1.996</td>\n",
       "      <td>0.970282</td>\n",
       "      <td>0.749814</td>\n",
       "      <td>0.958625</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.971035</td>\n",
       "      <td>0.996520</td>\n",
       "      <td>0.967853</td>\n",
       "      <td>0.747840</td>\n",
       "      <td>0.968272</td>\n",
       "      <td>0.734625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.969573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.958683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966830</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.963329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.686</td>\n",
       "      <td>0.964877</td>\n",
       "      <td>0.622708</td>\n",
       "      <td>0.956846</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.963998</td>\n",
       "      <td>0.971109</td>\n",
       "      <td>0.962407</td>\n",
       "      <td>0.316230</td>\n",
       "      <td>0.963828</td>\n",
       "      <td>0.541627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-2.562</td>\n",
       "      <td>0.961348</td>\n",
       "      <td>0.052771</td>\n",
       "      <td>0.951158</td>\n",
       "      <td>0.991600</td>\n",
       "      <td>0.955293</td>\n",
       "      <td>0.979945</td>\n",
       "      <td>0.953718</td>\n",
       "      <td>0.800910</td>\n",
       "      <td>0.958521</td>\n",
       "      <td>0.731011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2.323</td>\n",
       "      <td>0.955855</td>\n",
       "      <td>0.614955</td>\n",
       "      <td>0.948258</td>\n",
       "      <td>0.988600</td>\n",
       "      <td>0.953643</td>\n",
       "      <td>0.985563</td>\n",
       "      <td>0.952855</td>\n",
       "      <td>0.807670</td>\n",
       "      <td>0.955275</td>\n",
       "      <td>0.960060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.956628</td>\n",
       "      <td>0.771660</td>\n",
       "      <td>0.957982</td>\n",
       "      <td>0.742700</td>\n",
       "      <td>0.959183</td>\n",
       "      <td>0.993040</td>\n",
       "      <td>0.955877</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.955460</td>\n",
       "      <td>0.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-1.900</td>\n",
       "      <td>0.954632</td>\n",
       "      <td>0.699067</td>\n",
       "      <td>0.955924</td>\n",
       "      <td>0.812771</td>\n",
       "      <td>0.958037</td>\n",
       "      <td>0.977580</td>\n",
       "      <td>0.953916</td>\n",
       "      <td>0.855100</td>\n",
       "      <td>0.956712</td>\n",
       "      <td>0.960900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Delta  ReuterCredit  reuter_score  InvestCredit  invest_score  \\\n",
       "0    -4.060      0.973861      0.497530      0.976303      0.911567   \n",
       "1    14.400      0.949589      0.577730      0.931008      0.873125   \n",
       "2    24.410      0.945488      0.572960      0.937707      0.959300   \n",
       "3    -8.260      0.958241      0.058500      0.950460      0.982900   \n",
       "4    -6.770      0.968660      0.647170      0.957466      0.952050   \n",
       "5     5.170      0.971340      0.520433      0.961143      0.963560   \n",
       "6    20.880      0.951983      0.613570      0.950226      0.875420   \n",
       "7     0.620      0.958883      0.488550      0.957756      0.981425   \n",
       "8    30.050      0.955338      0.468512      0.949628      0.968350   \n",
       "9   -44.050      0.945884      0.447844      0.946424      0.988067   \n",
       "10   16.730      0.955978      0.587855      0.956641      0.978900   \n",
       "11    6.770      0.964131      0.563689      0.963630      0.921250   \n",
       "12   12.580      0.948454      0.115389      0.948204      0.938600   \n",
       "13    5.230      0.958082      0.881422      0.952601      0.604780   \n",
       "14   -2.620      0.965245      0.288322      0.962504      0.976400   \n",
       "15   16.840      0.962637      0.188200      0.959476      0.993720   \n",
       "16  -29.140      0.953324      0.579109      0.951744      0.991800   \n",
       "17   10.590      0.964751      0.707411      0.963961      0.741020   \n",
       "18   -2.480      0.972124      0.529630      0.968728      0.761230   \n",
       "19   -8.460      0.960749      0.080812      0.959436      0.802000   \n",
       "20   18.700      0.960695     -0.053078      0.963719      0.929357   \n",
       "21   30.260      0.955544      0.386140      0.957643      0.908040   \n",
       "22   60.290      0.949518      0.312682      0.950759      0.954340   \n",
       "23    1.000      0.962977      0.226213      0.964363      0.536350   \n",
       "24   -8.090      0.951986      0.000000      0.963993      0.972050   \n",
       "25   35.800      0.955422      0.000000      0.961097      0.981330   \n",
       "26   11.960      0.953823      0.061150      0.957761      0.977680   \n",
       "27   13.270      0.965274      0.619411      0.969990      0.952140   \n",
       "28  -56.210      0.962877      0.000000      0.968284      0.000000   \n",
       "29   28.850      0.950824      0.315725      0.955866      0.000000   \n",
       "..      ...           ...           ...           ...           ...   \n",
       "270   0.900      0.960695      0.424633      0.963719      0.985750   \n",
       "271   1.782      0.955544      0.244467      0.957643      0.973533   \n",
       "272   1.094      0.949518      0.451640      0.950759      0.412600   \n",
       "273  -0.975      0.962977      0.700250      0.964363      0.987650   \n",
       "274  -1.942      0.951986      0.639822      0.963993      0.986500   \n",
       "275   0.926      0.955422      0.811286      0.961097      0.991800   \n",
       "276  -1.696      0.953823      0.294720      0.957761      0.989000   \n",
       "277   0.464      0.965274      0.583217      0.969990      0.993200   \n",
       "278   2.011      0.962877      0.375360      0.968284      0.983233   \n",
       "279  -1.223      0.950824      0.000000      0.955866      0.000000   \n",
       "280   2.811      0.958651      0.000000      0.963487      0.000000   \n",
       "281   0.324      0.959980      0.000000      0.965414      0.000000   \n",
       "282  -0.710      0.961173      0.000000      0.965809      0.000000   \n",
       "283  -0.295      0.961302      0.000000      0.965732      0.000000   \n",
       "284  -0.099      0.946764      0.000000      0.949788      0.000000   \n",
       "285  -0.957      0.948680      0.386709      0.951733      0.977900   \n",
       "286  -1.102      0.962394      0.743711      0.958318      0.990800   \n",
       "287   0.079      0.961949      0.412786      0.956678      0.000000   \n",
       "288  -0.278      0.970783      0.000000      0.956876      0.000000   \n",
       "289  -1.182      0.968381      0.000000      0.957067      0.897750   \n",
       "290  -1.052      0.966957      0.635550      0.959131      0.000000   \n",
       "291   1.588      0.973950      0.000000      0.962424      0.000000   \n",
       "292   1.996      0.970282      0.749814      0.958625      0.996400   \n",
       "293   0.735      0.969573      0.000000      0.958683      0.000000   \n",
       "294   0.705      0.963329      0.000000      0.954553      0.000000   \n",
       "295   0.686      0.964877      0.622708      0.956846      0.974100   \n",
       "296  -2.562      0.961348      0.052771      0.951158      0.991600   \n",
       "297   2.323      0.955855      0.614955      0.948258      0.988600   \n",
       "298   0.735      0.956628      0.771660      0.957982      0.742700   \n",
       "299  -1.900      0.954632      0.699067      0.955924      0.812771   \n",
       "\n",
       "     FoolCredit  fool_score  CnbcCredit  cnbc_score  MarketCredit  \\\n",
       "0      0.973861    0.984530    0.973861    0.924140      0.977183   \n",
       "1      0.949589    0.992450    0.949589    0.745560      0.953855   \n",
       "2      0.945488    0.990220    0.945488    0.942220      0.947621   \n",
       "3      0.952755    0.983064    0.957221    0.957580      0.958288   \n",
       "4      0.961451    0.977010    0.968150    0.985720      0.968683   \n",
       "5      0.969940    0.979464    0.973290    0.970120      0.973557   \n",
       "6      0.951283    0.983930    0.952958    0.728020      0.954236   \n",
       "7      0.958191    0.976930    0.959028    0.851680      0.960240   \n",
       "8      0.954649    0.983340    0.955067    0.317280      0.956246   \n",
       "9      0.945540    0.936760    0.945749    0.508890      0.946911   \n",
       "10     0.956920    0.903220    0.956014    0.935950      0.954971   \n",
       "11     0.963369    0.798460    0.964145    0.720800      0.962853   \n",
       "12     0.948073    0.762780    0.948461    0.685980      0.948273   \n",
       "13     0.957892    0.991100    0.958086    0.922800      0.958450   \n",
       "14     0.965150    0.995150    0.965247    0.862680      0.965887   \n",
       "15     0.964096    0.994780    0.962638    0.658450      0.964465   \n",
       "16     0.954054    0.800550    0.953325    0.746890      0.954238   \n",
       "17     0.963285    0.993320    0.964752    0.724320      0.965208   \n",
       "18     0.971391    0.994190    0.972124    0.696450      0.972353   \n",
       "19     0.960767    0.991370    0.961134    0.968690      0.960863   \n",
       "20     0.957933    0.880200    0.961080    0.698273      0.957981   \n",
       "21     0.954011    0.969660    0.957065    0.650780      0.953938   \n",
       "22     0.948030    0.966540    0.951038    0.586990      0.947897   \n",
       "23     0.962233    0.986918    0.963737    0.946600      0.951843   \n",
       "24     0.951614    0.000000    0.952366    0.000000      0.949935   \n",
       "25     0.955229    0.000000    0.952130    0.624050      0.953573   \n",
       "26     0.948473    0.000000    0.947794    0.000000      0.950412   \n",
       "27     0.961259    0.000000    0.964578    0.888620      0.964585   \n",
       "28     0.950795    0.995600    0.951528    0.418660      0.963225   \n",
       "29     0.943408    0.000000    0.943841    0.000000      0.950969   \n",
       "..          ...         ...         ...         ...           ...   \n",
       "270    0.957933    0.801800    0.961080    0.484925      0.957981   \n",
       "271    0.954011    0.990600    0.957065    0.627280      0.953938   \n",
       "272    0.948030    0.755100    0.951038    0.602560      0.947897   \n",
       "273    0.962233    0.982400    0.963737    0.711222      0.951843   \n",
       "274    0.951614    0.981525    0.952366    0.864486      0.949935   \n",
       "275    0.955229    0.997900    0.952130    0.740000      0.953573   \n",
       "276    0.948473    0.991525    0.947794    0.859586      0.950412   \n",
       "277    0.961259    0.864871    0.964578    0.718260      0.964585   \n",
       "278    0.950795    0.792410    0.951528    0.200440      0.963225   \n",
       "279    0.943408    0.000000    0.943841    0.000000      0.950969   \n",
       "280    0.954938    0.000000    0.952346    0.000000      0.958192   \n",
       "281    0.958124    0.000000    0.956828    0.000000      0.959751   \n",
       "282    0.959278    0.000000    0.957958    0.000000      0.961239   \n",
       "283    0.959332    0.000000    0.957999    0.000000      0.961460   \n",
       "284    0.946854    0.000000    0.943445    0.000000      0.946366   \n",
       "285    0.948636    0.727020    0.944258    0.274880      0.948479   \n",
       "286    0.964341    0.989433    0.957967    0.377650      0.962002   \n",
       "287    0.959344    0.000000    0.953220    0.000000      0.957371   \n",
       "288    0.967330    0.000000    0.964268    0.000000      0.964005   \n",
       "289    0.966314    0.926580    0.964720    0.251460      0.963391   \n",
       "290    0.968095    0.853040    0.965569    0.462050      0.964240   \n",
       "291    0.973446    0.000000    0.970385    0.000000      0.970257   \n",
       "292    0.971035    0.996520    0.967853    0.747840      0.968272   \n",
       "293    0.970590    0.000000    0.967440    0.000000      0.966830   \n",
       "294    0.966267    0.000000    0.963117    0.000000      0.960213   \n",
       "295    0.963998    0.971109    0.962407    0.316230      0.963828   \n",
       "296    0.955293    0.979945    0.953718    0.800910      0.958521   \n",
       "297    0.953643    0.985563    0.952855    0.807670      0.955275   \n",
       "298    0.959183    0.993040    0.955877    0.382500      0.955460   \n",
       "299    0.958037    0.977580    0.953916    0.855100      0.956712   \n",
       "\n",
       "     market_score  \n",
       "0        0.895209  \n",
       "1        0.488955  \n",
       "2        0.421736  \n",
       "3        0.694864  \n",
       "4        0.970160  \n",
       "5        0.727500  \n",
       "6        0.736736  \n",
       "7        0.635125  \n",
       "8        0.691500  \n",
       "9        0.424650  \n",
       "10       0.751375  \n",
       "11       0.937718  \n",
       "12       0.860000  \n",
       "13       0.909411  \n",
       "14       0.896427  \n",
       "15       0.510431  \n",
       "16       0.651292  \n",
       "17       0.370440  \n",
       "18       0.334078  \n",
       "19       0.478400  \n",
       "20       0.671618  \n",
       "21       0.401945  \n",
       "22       0.677808  \n",
       "23       0.000000  \n",
       "24       0.583620  \n",
       "25       0.000000  \n",
       "26       0.790800  \n",
       "27       0.960550  \n",
       "28       0.000000  \n",
       "29       0.765990  \n",
       "..            ...  \n",
       "270      0.994100  \n",
       "271      0.979720  \n",
       "272      0.661886  \n",
       "273      0.992100  \n",
       "274      0.994050  \n",
       "275      0.747086  \n",
       "276      0.988320  \n",
       "277      0.931250  \n",
       "278      0.631100  \n",
       "279      0.000000  \n",
       "280      0.000000  \n",
       "281      0.000000  \n",
       "282      0.000000  \n",
       "283      0.000000  \n",
       "284      0.000000  \n",
       "285      0.561629  \n",
       "286      0.872740  \n",
       "287      0.000000  \n",
       "288      0.000000  \n",
       "289      0.127437  \n",
       "290      0.375300  \n",
       "291      0.000000  \n",
       "292      0.734625  \n",
       "293      0.000000  \n",
       "294      0.000000  \n",
       "295      0.541627  \n",
       "296      0.731011  \n",
       "297      0.960060  \n",
       "298      0.960700  \n",
       "299      0.960900  \n",
       "\n",
       "[300 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 268,801\n",
      "Trainable params: 268,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    " optimizer='adam',\n",
    " metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216 samples, validate on 24 samples\n",
      "Epoch 1/500\n",
      "216/216 [==============================] - 0s 2ms/step - loss: 53.7152 - mean_squared_error: 53.7152 - val_loss: 53.1053 - val_mean_squared_error: 53.1053\n",
      "Epoch 2/500\n",
      "216/216 [==============================] - 0s 54us/step - loss: 60.4205 - mean_squared_error: 60.4205 - val_loss: 51.3457 - val_mean_squared_error: 51.3457\n",
      "Epoch 3/500\n",
      "216/216 [==============================] - 0s 56us/step - loss: 57.0619 - mean_squared_error: 57.0619 - val_loss: 51.5697 - val_mean_squared_error: 51.5697\n",
      "Epoch 4/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 54.6239 - mean_squared_error: 54.6239 - val_loss: 49.9582 - val_mean_squared_error: 49.9582\n",
      "Epoch 5/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 55.7060 - mean_squared_error: 55.7060 - val_loss: 52.6389 - val_mean_squared_error: 52.6389\n",
      "Epoch 6/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 55.6348 - mean_squared_error: 55.6348 - val_loss: 55.3869 - val_mean_squared_error: 55.3869\n",
      "Epoch 7/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 59.3612 - mean_squared_error: 59.3612 - val_loss: 57.2482 - val_mean_squared_error: 57.2482\n",
      "Epoch 8/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 55.1459 - mean_squared_error: 55.1459 - val_loss: 57.2659 - val_mean_squared_error: 57.2659\n",
      "Epoch 9/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 53.9761 - mean_squared_error: 53.9761 - val_loss: 55.6202 - val_mean_squared_error: 55.6202\n",
      "Epoch 10/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 54.8701 - mean_squared_error: 54.8701 - val_loss: 55.9287 - val_mean_squared_error: 55.9287\n",
      "Epoch 11/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 55.0425 - mean_squared_error: 55.0425 - val_loss: 56.0310 - val_mean_squared_error: 56.0310\n",
      "Epoch 12/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 52.8367 - mean_squared_error: 52.8367 - val_loss: 56.9284 - val_mean_squared_error: 56.9284\n",
      "Epoch 13/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 54.7232 - mean_squared_error: 54.7232 - val_loss: 58.6250 - val_mean_squared_error: 58.6250\n",
      "Epoch 14/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 54.8698 - mean_squared_error: 54.8698 - val_loss: 58.9078 - val_mean_squared_error: 58.9078\n",
      "Epoch 15/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 54.1147 - mean_squared_error: 54.1147 - val_loss: 57.7471 - val_mean_squared_error: 57.7471\n",
      "Epoch 16/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 52.9225 - mean_squared_error: 52.9225 - val_loss: 55.2228 - val_mean_squared_error: 55.2228\n",
      "Epoch 17/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 52.1566 - mean_squared_error: 52.1566 - val_loss: 53.8014 - val_mean_squared_error: 53.8014\n",
      "Epoch 18/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 52.3807 - mean_squared_error: 52.3807 - val_loss: 53.7223 - val_mean_squared_error: 53.7223\n",
      "Epoch 19/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 53.0530 - mean_squared_error: 53.0530 - val_loss: 53.4938 - val_mean_squared_error: 53.4938\n",
      "Epoch 20/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 53.0909 - mean_squared_error: 53.0909 - val_loss: 53.8293 - val_mean_squared_error: 53.8293\n",
      "Epoch 21/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 50.7949 - mean_squared_error: 50.7949 - val_loss: 54.0079 - val_mean_squared_error: 54.0079\n",
      "Epoch 22/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 53.9074 - mean_squared_error: 53.9074 - val_loss: 53.7760 - val_mean_squared_error: 53.7760\n",
      "Epoch 23/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 53.9649 - mean_squared_error: 53.9649 - val_loss: 54.8119 - val_mean_squared_error: 54.8119\n",
      "Epoch 24/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 53.7171 - mean_squared_error: 53.7171 - val_loss: 55.1657 - val_mean_squared_error: 55.1657\n",
      "Epoch 25/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 55.7770 - mean_squared_error: 55.7770 - val_loss: 54.7490 - val_mean_squared_error: 54.7490\n",
      "Epoch 26/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 51.9579 - mean_squared_error: 51.9579 - val_loss: 55.6746 - val_mean_squared_error: 55.6746\n",
      "Epoch 27/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 53.9065 - mean_squared_error: 53.9065 - val_loss: 55.3497 - val_mean_squared_error: 55.3497\n",
      "Epoch 28/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 54.0819 - mean_squared_error: 54.0819 - val_loss: 53.9803 - val_mean_squared_error: 53.9803\n",
      "Epoch 29/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 53.1304 - mean_squared_error: 53.1304 - val_loss: 52.9391 - val_mean_squared_error: 52.9391\n",
      "Epoch 30/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 53.5986 - mean_squared_error: 53.5986 - val_loss: 53.3722 - val_mean_squared_error: 53.3722\n",
      "Epoch 31/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 54.5011 - mean_squared_error: 54.5011 - val_loss: 54.0103 - val_mean_squared_error: 54.0103\n",
      "Epoch 32/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 56.2499 - mean_squared_error: 56.2499 - val_loss: 56.5006 - val_mean_squared_error: 56.5006\n",
      "Epoch 33/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 53.5762 - mean_squared_error: 53.5762 - val_loss: 54.0371 - val_mean_squared_error: 54.0371\n",
      "Epoch 34/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 54.4529 - mean_squared_error: 54.4529 - val_loss: 55.3251 - val_mean_squared_error: 55.3251\n",
      "Epoch 35/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 53.4359 - mean_squared_error: 53.4359 - val_loss: 54.9053 - val_mean_squared_error: 54.9053\n",
      "Epoch 36/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 56.4725 - mean_squared_error: 56.4725 - val_loss: 56.9723 - val_mean_squared_error: 56.9723\n",
      "Epoch 37/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 54.0357 - mean_squared_error: 54.0357 - val_loss: 56.4927 - val_mean_squared_error: 56.4927\n",
      "Epoch 38/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 55.6947 - mean_squared_error: 55.6947 - val_loss: 57.2277 - val_mean_squared_error: 57.2277\n",
      "Epoch 39/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 56.6520 - mean_squared_error: 56.6520 - val_loss: 54.1930 - val_mean_squared_error: 54.1930\n",
      "Epoch 40/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 53.2296 - mean_squared_error: 53.2296 - val_loss: 52.9909 - val_mean_squared_error: 52.9909\n",
      "Epoch 41/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 53.9122 - mean_squared_error: 53.9122 - val_loss: 52.9517 - val_mean_squared_error: 52.9517\n",
      "Epoch 42/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 53.6016 - mean_squared_error: 53.6016 - val_loss: 53.6890 - val_mean_squared_error: 53.6890\n",
      "Epoch 43/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 54.7718 - mean_squared_error: 54.7718 - val_loss: 55.6338 - val_mean_squared_error: 55.6338\n",
      "Epoch 44/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 51.4798 - mean_squared_error: 51.4798 - val_loss: 55.1505 - val_mean_squared_error: 55.1505\n",
      "Epoch 45/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 52.5640 - mean_squared_error: 52.5640 - val_loss: 54.2610 - val_mean_squared_error: 54.2610\n",
      "Epoch 46/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 51.1827 - mean_squared_error: 51.1827 - val_loss: 54.6894 - val_mean_squared_error: 54.6894\n",
      "Epoch 47/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 52.7584 - mean_squared_error: 52.7584 - val_loss: 53.8007 - val_mean_squared_error: 53.8007\n",
      "Epoch 48/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 51.4948 - mean_squared_error: 51.4948 - val_loss: 54.9872 - val_mean_squared_error: 54.9872\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 44us/step - loss: 53.0013 - mean_squared_error: 53.0013 - val_loss: 56.0191 - val_mean_squared_error: 56.0191\n",
      "Epoch 50/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 54.5555 - mean_squared_error: 54.5555 - val_loss: 56.0677 - val_mean_squared_error: 56.0677\n",
      "Epoch 51/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 53.6531 - mean_squared_error: 53.6531 - val_loss: 55.3669 - val_mean_squared_error: 55.3669\n",
      "Epoch 52/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 48.4677 - mean_squared_error: 48.4677 - val_loss: 54.7854 - val_mean_squared_error: 54.7854\n",
      "Epoch 53/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 51.7486 - mean_squared_error: 51.7486 - val_loss: 54.8847 - val_mean_squared_error: 54.8847\n",
      "Epoch 54/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 55.7641 - mean_squared_error: 55.7641 - val_loss: 54.5363 - val_mean_squared_error: 54.5363\n",
      "Epoch 55/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 48.1309 - mean_squared_error: 48.1309 - val_loss: 54.2664 - val_mean_squared_error: 54.2664\n",
      "Epoch 56/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 51.3711 - mean_squared_error: 51.3711 - val_loss: 53.8442 - val_mean_squared_error: 53.8442\n",
      "Epoch 57/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 50.9049 - mean_squared_error: 50.9049 - val_loss: 54.0050 - val_mean_squared_error: 54.0050\n",
      "Epoch 58/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 53.0827 - mean_squared_error: 53.0827 - val_loss: 54.9370 - val_mean_squared_error: 54.9370\n",
      "Epoch 59/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 50.2005 - mean_squared_error: 50.2005 - val_loss: 56.9536 - val_mean_squared_error: 56.9536\n",
      "Epoch 60/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 56.4929 - mean_squared_error: 56.4929 - val_loss: 58.1774 - val_mean_squared_error: 58.1774\n",
      "Epoch 61/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 52.5627 - mean_squared_error: 52.5627 - val_loss: 56.6641 - val_mean_squared_error: 56.6641\n",
      "Epoch 62/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 52.9917 - mean_squared_error: 52.9917 - val_loss: 55.3529 - val_mean_squared_error: 55.3529\n",
      "Epoch 63/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 50.8562 - mean_squared_error: 50.8562 - val_loss: 54.0175 - val_mean_squared_error: 54.0175\n",
      "Epoch 64/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 52.8002 - mean_squared_error: 52.8002 - val_loss: 53.9038 - val_mean_squared_error: 53.9038\n",
      "Epoch 65/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 53.1652 - mean_squared_error: 53.1652 - val_loss: 53.0299 - val_mean_squared_error: 53.0299\n",
      "Epoch 66/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 52.0140 - mean_squared_error: 52.0140 - val_loss: 52.6880 - val_mean_squared_error: 52.6880\n",
      "Epoch 67/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 52.6166 - mean_squared_error: 52.6166 - val_loss: 52.9845 - val_mean_squared_error: 52.9845\n",
      "Epoch 68/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 50.3633 - mean_squared_error: 50.3633 - val_loss: 54.6052 - val_mean_squared_error: 54.6052\n",
      "Epoch 69/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 51.6626 - mean_squared_error: 51.6626 - val_loss: 55.2986 - val_mean_squared_error: 55.2986\n",
      "Epoch 70/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 54.6325 - mean_squared_error: 54.6325 - val_loss: 55.7673 - val_mean_squared_error: 55.7673\n",
      "Epoch 71/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 54.1042 - mean_squared_error: 54.1042 - val_loss: 55.8327 - val_mean_squared_error: 55.8327\n",
      "Epoch 72/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 54.4824 - mean_squared_error: 54.4824 - val_loss: 56.5533 - val_mean_squared_error: 56.5533\n",
      "Epoch 73/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 50.9871 - mean_squared_error: 50.9871 - val_loss: 56.3798 - val_mean_squared_error: 56.3798\n",
      "Epoch 74/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 49.1448 - mean_squared_error: 49.1448 - val_loss: 55.5056 - val_mean_squared_error: 55.5056\n",
      "Epoch 75/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 53.2126 - mean_squared_error: 53.2126 - val_loss: 53.9905 - val_mean_squared_error: 53.9905\n",
      "Epoch 76/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 51.5872 - mean_squared_error: 51.5872 - val_loss: 53.5260 - val_mean_squared_error: 53.5260\n",
      "Epoch 77/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 53.0531 - mean_squared_error: 53.0531 - val_loss: 53.8706 - val_mean_squared_error: 53.8706\n",
      "Epoch 78/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 52.3884 - mean_squared_error: 52.3884 - val_loss: 54.3363 - val_mean_squared_error: 54.3363\n",
      "Epoch 79/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 51.7301 - mean_squared_error: 51.7301 - val_loss: 53.5439 - val_mean_squared_error: 53.5439\n",
      "Epoch 80/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 49.9643 - mean_squared_error: 49.9643 - val_loss: 54.1501 - val_mean_squared_error: 54.1501\n",
      "Epoch 81/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 51.7894 - mean_squared_error: 51.7894 - val_loss: 54.6311 - val_mean_squared_error: 54.6311\n",
      "Epoch 82/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 54.5689 - mean_squared_error: 54.5689 - val_loss: 55.5457 - val_mean_squared_error: 55.5457\n",
      "Epoch 83/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 50.4115 - mean_squared_error: 50.4115 - val_loss: 57.6708 - val_mean_squared_error: 57.6708\n",
      "Epoch 84/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 52.4206 - mean_squared_error: 52.4206 - val_loss: 55.6090 - val_mean_squared_error: 55.6090\n",
      "Epoch 85/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 52.2109 - mean_squared_error: 52.2109 - val_loss: 52.9475 - val_mean_squared_error: 52.9475\n",
      "Epoch 86/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 51.3301 - mean_squared_error: 51.3301 - val_loss: 51.5431 - val_mean_squared_error: 51.5431\n",
      "Epoch 87/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 52.7436 - mean_squared_error: 52.7436 - val_loss: 52.4214 - val_mean_squared_error: 52.4214\n",
      "Epoch 88/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 50.9200 - mean_squared_error: 50.9200 - val_loss: 58.3714 - val_mean_squared_error: 58.3714\n",
      "Epoch 89/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 52.4459 - mean_squared_error: 52.4459 - val_loss: 62.1774 - val_mean_squared_error: 62.1774\n",
      "Epoch 90/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 55.7821 - mean_squared_error: 55.7821 - val_loss: 61.1343 - val_mean_squared_error: 61.1343\n",
      "Epoch 91/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 52.9412 - mean_squared_error: 52.9412 - val_loss: 57.3735 - val_mean_squared_error: 57.3735\n",
      "Epoch 92/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 57.5584 - mean_squared_error: 57.5584 - val_loss: 54.6944 - val_mean_squared_error: 54.6944\n",
      "Epoch 93/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 52.9347 - mean_squared_error: 52.9347 - val_loss: 53.7633 - val_mean_squared_error: 53.7633\n",
      "Epoch 94/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 52.6453 - mean_squared_error: 52.6453 - val_loss: 55.6024 - val_mean_squared_error: 55.6024\n",
      "Epoch 95/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 53.4243 - mean_squared_error: 53.4243 - val_loss: 57.5403 - val_mean_squared_error: 57.5403\n",
      "Epoch 96/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 54.1894 - mean_squared_error: 54.1894 - val_loss: 56.9166 - val_mean_squared_error: 56.9166\n",
      "Epoch 97/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 40us/step - loss: 50.3682 - mean_squared_error: 50.3682 - val_loss: 54.6179 - val_mean_squared_error: 54.6179\n",
      "Epoch 98/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 50.0843 - mean_squared_error: 50.0843 - val_loss: 54.4607 - val_mean_squared_error: 54.4607\n",
      "Epoch 99/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 51.3287 - mean_squared_error: 51.3287 - val_loss: 56.1029 - val_mean_squared_error: 56.1029\n",
      "Epoch 100/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 50.1499 - mean_squared_error: 50.1499 - val_loss: 58.2610 - val_mean_squared_error: 58.2610\n",
      "Epoch 101/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 52.8973 - mean_squared_error: 52.8973 - val_loss: 60.8647 - val_mean_squared_error: 60.8647\n",
      "Epoch 102/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 54.1905 - mean_squared_error: 54.1905 - val_loss: 56.9133 - val_mean_squared_error: 56.9133\n",
      "Epoch 103/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 51.1919 - mean_squared_error: 51.1919 - val_loss: 52.8297 - val_mean_squared_error: 52.8297\n",
      "Epoch 104/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 49.8680 - mean_squared_error: 49.8680 - val_loss: 52.2838 - val_mean_squared_error: 52.2838\n",
      "Epoch 105/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 50.3540 - mean_squared_error: 50.3540 - val_loss: 52.9541 - val_mean_squared_error: 52.9541\n",
      "Epoch 106/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 50.4785 - mean_squared_error: 50.4785 - val_loss: 55.4685 - val_mean_squared_error: 55.4685\n",
      "Epoch 107/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 47.5404 - mean_squared_error: 47.5404 - val_loss: 59.3504 - val_mean_squared_error: 59.3504\n",
      "Epoch 108/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 53.4686 - mean_squared_error: 53.4686 - val_loss: 60.1664 - val_mean_squared_error: 60.1664\n",
      "Epoch 109/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 49.9271 - mean_squared_error: 49.9271 - val_loss: 58.9535 - val_mean_squared_error: 58.9535\n",
      "Epoch 110/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 50.0191 - mean_squared_error: 50.0191 - val_loss: 55.5818 - val_mean_squared_error: 55.5818\n",
      "Epoch 111/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 49.0898 - mean_squared_error: 49.0898 - val_loss: 53.2555 - val_mean_squared_error: 53.2555\n",
      "Epoch 112/500\n",
      "216/216 [==============================] - 0s 55us/step - loss: 49.7439 - mean_squared_error: 49.7439 - val_loss: 54.4529 - val_mean_squared_error: 54.4529\n",
      "Epoch 113/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 53.5619 - mean_squared_error: 53.5619 - val_loss: 55.6664 - val_mean_squared_error: 55.6664\n",
      "Epoch 114/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 47.2730 - mean_squared_error: 47.2730 - val_loss: 56.6918 - val_mean_squared_error: 56.6918\n",
      "Epoch 115/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 49.9401 - mean_squared_error: 49.9401 - val_loss: 56.3894 - val_mean_squared_error: 56.3894\n",
      "Epoch 116/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 49.4218 - mean_squared_error: 49.4218 - val_loss: 55.8786 - val_mean_squared_error: 55.8786\n",
      "Epoch 117/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 49.8345 - mean_squared_error: 49.8345 - val_loss: 55.3859 - val_mean_squared_error: 55.3859\n",
      "Epoch 118/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 48.6181 - mean_squared_error: 48.6181 - val_loss: 56.1379 - val_mean_squared_error: 56.1379\n",
      "Epoch 119/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 51.1530 - mean_squared_error: 51.1530 - val_loss: 54.8445 - val_mean_squared_error: 54.8445\n",
      "Epoch 120/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 51.6580 - mean_squared_error: 51.6580 - val_loss: 55.6007 - val_mean_squared_error: 55.6007\n",
      "Epoch 121/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 51.0127 - mean_squared_error: 51.0127 - val_loss: 56.0028 - val_mean_squared_error: 56.0028\n",
      "Epoch 122/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 54.2807 - mean_squared_error: 54.2807 - val_loss: 56.9390 - val_mean_squared_error: 56.9390\n",
      "Epoch 123/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 49.8145 - mean_squared_error: 49.8145 - val_loss: 59.5492 - val_mean_squared_error: 59.5492\n",
      "Epoch 124/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 53.5199 - mean_squared_error: 53.5199 - val_loss: 60.1592 - val_mean_squared_error: 60.1592\n",
      "Epoch 125/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 48.1075 - mean_squared_error: 48.1075 - val_loss: 59.5218 - val_mean_squared_error: 59.5218\n",
      "Epoch 126/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 51.3242 - mean_squared_error: 51.3242 - val_loss: 55.7247 - val_mean_squared_error: 55.7247\n",
      "Epoch 127/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 50.0088 - mean_squared_error: 50.0088 - val_loss: 54.9702 - val_mean_squared_error: 54.9702\n",
      "Epoch 128/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 49.8588 - mean_squared_error: 49.8588 - val_loss: 55.5753 - val_mean_squared_error: 55.5753\n",
      "Epoch 129/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 49.4151 - mean_squared_error: 49.4151 - val_loss: 58.1515 - val_mean_squared_error: 58.1515\n",
      "Epoch 130/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 56.1387 - mean_squared_error: 56.1387 - val_loss: 55.2878 - val_mean_squared_error: 55.2878\n",
      "Epoch 131/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 49.0622 - mean_squared_error: 49.0622 - val_loss: 54.5860 - val_mean_squared_error: 54.5860\n",
      "Epoch 132/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 46.7084 - mean_squared_error: 46.7084 - val_loss: 53.0263 - val_mean_squared_error: 53.0263\n",
      "Epoch 133/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 49.1445 - mean_squared_error: 49.1445 - val_loss: 55.7444 - val_mean_squared_error: 55.7444\n",
      "Epoch 134/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 49.1687 - mean_squared_error: 49.1687 - val_loss: 57.5974 - val_mean_squared_error: 57.5974\n",
      "Epoch 135/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 48.8230 - mean_squared_error: 48.8230 - val_loss: 57.7142 - val_mean_squared_error: 57.7142\n",
      "Epoch 136/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 47.5830 - mean_squared_error: 47.5830 - val_loss: 57.2006 - val_mean_squared_error: 57.2006\n",
      "Epoch 137/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 47.8290 - mean_squared_error: 47.8290 - val_loss: 57.0014 - val_mean_squared_error: 57.0014\n",
      "Epoch 138/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 46.4364 - mean_squared_error: 46.4364 - val_loss: 56.3511 - val_mean_squared_error: 56.3511\n",
      "Epoch 139/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 50.6253 - mean_squared_error: 50.6253 - val_loss: 55.7230 - val_mean_squared_error: 55.7230\n",
      "Epoch 140/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 48.9226 - mean_squared_error: 48.9226 - val_loss: 56.3734 - val_mean_squared_error: 56.3734\n",
      "Epoch 141/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 48.0974 - mean_squared_error: 48.0974 - val_loss: 57.8685 - val_mean_squared_error: 57.8685\n",
      "Epoch 142/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 49.0930 - mean_squared_error: 49.0930 - val_loss: 57.6581 - val_mean_squared_error: 57.6581\n",
      "Epoch 143/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 49.7853 - mean_squared_error: 49.7853 - val_loss: 55.1109 - val_mean_squared_error: 55.1109\n",
      "Epoch 144/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 47.2550 - mean_squared_error: 47.2550 - val_loss: 54.6954 - val_mean_squared_error: 54.6954\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 47us/step - loss: 46.7046 - mean_squared_error: 46.7046 - val_loss: 56.2978 - val_mean_squared_error: 56.2978\n",
      "Epoch 146/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 48.6127 - mean_squared_error: 48.6127 - val_loss: 56.6611 - val_mean_squared_error: 56.6611\n",
      "Epoch 147/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 46.3692 - mean_squared_error: 46.3692 - val_loss: 58.8638 - val_mean_squared_error: 58.8638\n",
      "Epoch 148/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 50.7722 - mean_squared_error: 50.7722 - val_loss: 58.5795 - val_mean_squared_error: 58.5795\n",
      "Epoch 149/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 48.5142 - mean_squared_error: 48.5142 - val_loss: 55.4914 - val_mean_squared_error: 55.4914\n",
      "Epoch 150/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 47.8198 - mean_squared_error: 47.8198 - val_loss: 55.8863 - val_mean_squared_error: 55.8863\n",
      "Epoch 151/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 48.5747 - mean_squared_error: 48.5747 - val_loss: 57.0389 - val_mean_squared_error: 57.0389\n",
      "Epoch 152/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 50.5373 - mean_squared_error: 50.5373 - val_loss: 59.0979 - val_mean_squared_error: 59.0979\n",
      "Epoch 153/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 49.3410 - mean_squared_error: 49.3410 - val_loss: 59.7792 - val_mean_squared_error: 59.7792\n",
      "Epoch 154/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 46.3498 - mean_squared_error: 46.3498 - val_loss: 58.6245 - val_mean_squared_error: 58.6245\n",
      "Epoch 155/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 46.1551 - mean_squared_error: 46.1551 - val_loss: 59.7245 - val_mean_squared_error: 59.7245\n",
      "Epoch 156/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 51.4546 - mean_squared_error: 51.4546 - val_loss: 59.2416 - val_mean_squared_error: 59.2416\n",
      "Epoch 157/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 46.5339 - mean_squared_error: 46.5339 - val_loss: 63.1636 - val_mean_squared_error: 63.1636\n",
      "Epoch 158/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 48.5055 - mean_squared_error: 48.5055 - val_loss: 59.5933 - val_mean_squared_error: 59.5933\n",
      "Epoch 159/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 52.0421 - mean_squared_error: 52.0421 - val_loss: 54.5300 - val_mean_squared_error: 54.5300\n",
      "Epoch 160/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 48.8467 - mean_squared_error: 48.8467 - val_loss: 54.6471 - val_mean_squared_error: 54.6471\n",
      "Epoch 161/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 51.9519 - mean_squared_error: 51.9519 - val_loss: 58.5401 - val_mean_squared_error: 58.5401\n",
      "Epoch 162/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 47.0916 - mean_squared_error: 47.0916 - val_loss: 62.1629 - val_mean_squared_error: 62.1629\n",
      "Epoch 163/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 49.8403 - mean_squared_error: 49.8403 - val_loss: 63.7191 - val_mean_squared_error: 63.7191\n",
      "Epoch 164/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 52.8796 - mean_squared_error: 52.8796 - val_loss: 60.2323 - val_mean_squared_error: 60.2323\n",
      "Epoch 165/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 47.0276 - mean_squared_error: 47.0276 - val_loss: 57.5391 - val_mean_squared_error: 57.5391\n",
      "Epoch 166/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 46.7887 - mean_squared_error: 46.7887 - val_loss: 56.0932 - val_mean_squared_error: 56.0932\n",
      "Epoch 167/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 49.0760 - mean_squared_error: 49.0760 - val_loss: 55.4617 - val_mean_squared_error: 55.4617\n",
      "Epoch 168/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 50.0119 - mean_squared_error: 50.0119 - val_loss: 58.1175 - val_mean_squared_error: 58.1175\n",
      "Epoch 169/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 50.7544 - mean_squared_error: 50.7544 - val_loss: 59.5359 - val_mean_squared_error: 59.5359\n",
      "Epoch 170/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 51.3558 - mean_squared_error: 51.3558 - val_loss: 57.1119 - val_mean_squared_error: 57.1119\n",
      "Epoch 171/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 47.8097 - mean_squared_error: 47.8097 - val_loss: 55.0028 - val_mean_squared_error: 55.0028\n",
      "Epoch 172/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 51.4322 - mean_squared_error: 51.4322 - val_loss: 55.2533 - val_mean_squared_error: 55.2533\n",
      "Epoch 173/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 49.4382 - mean_squared_error: 49.4382 - val_loss: 58.1774 - val_mean_squared_error: 58.1774\n",
      "Epoch 174/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 46.0119 - mean_squared_error: 46.0119 - val_loss: 58.1015 - val_mean_squared_error: 58.1015\n",
      "Epoch 175/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 46.3315 - mean_squared_error: 46.3315 - val_loss: 58.0326 - val_mean_squared_error: 58.0326\n",
      "Epoch 176/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 48.9440 - mean_squared_error: 48.9440 - val_loss: 56.0593 - val_mean_squared_error: 56.0593\n",
      "Epoch 177/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 47.7743 - mean_squared_error: 47.7743 - val_loss: 55.6382 - val_mean_squared_error: 55.6382\n",
      "Epoch 178/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 47.6537 - mean_squared_error: 47.6537 - val_loss: 55.5799 - val_mean_squared_error: 55.5799\n",
      "Epoch 179/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 47.8714 - mean_squared_error: 47.8714 - val_loss: 55.5132 - val_mean_squared_error: 55.5132\n",
      "Epoch 180/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 46.9804 - mean_squared_error: 46.9804 - val_loss: 57.9441 - val_mean_squared_error: 57.9441\n",
      "Epoch 181/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 51.5993 - mean_squared_error: 51.5993 - val_loss: 61.0816 - val_mean_squared_error: 61.0816\n",
      "Epoch 182/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 49.3250 - mean_squared_error: 49.3250 - val_loss: 59.6501 - val_mean_squared_error: 59.6501\n",
      "Epoch 183/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 47.8574 - mean_squared_error: 47.8574 - val_loss: 58.0153 - val_mean_squared_error: 58.0153\n",
      "Epoch 184/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 48.3423 - mean_squared_error: 48.3423 - val_loss: 57.9959 - val_mean_squared_error: 57.9959\n",
      "Epoch 185/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 47.5227 - mean_squared_error: 47.5227 - val_loss: 58.4521 - val_mean_squared_error: 58.4521\n",
      "Epoch 186/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 47.6138 - mean_squared_error: 47.6138 - val_loss: 58.9570 - val_mean_squared_error: 58.9570\n",
      "Epoch 187/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 49.1689 - mean_squared_error: 49.1689 - val_loss: 57.5002 - val_mean_squared_error: 57.5002\n",
      "Epoch 188/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 47.8642 - mean_squared_error: 47.8642 - val_loss: 54.8824 - val_mean_squared_error: 54.8824\n",
      "Epoch 189/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 49.6912 - mean_squared_error: 49.6912 - val_loss: 54.9436 - val_mean_squared_error: 54.9436\n",
      "Epoch 190/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 46.6675 - mean_squared_error: 46.6675 - val_loss: 54.9112 - val_mean_squared_error: 54.9112\n",
      "Epoch 191/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 48.4631 - mean_squared_error: 48.4631 - val_loss: 55.5162 - val_mean_squared_error: 55.5162\n",
      "Epoch 192/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 47.2972 - mean_squared_error: 47.2972 - val_loss: 57.2521 - val_mean_squared_error: 57.2521\n",
      "Epoch 193/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 44us/step - loss: 47.4369 - mean_squared_error: 47.4369 - val_loss: 58.6157 - val_mean_squared_error: 58.6157\n",
      "Epoch 194/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 46.0830 - mean_squared_error: 46.0830 - val_loss: 59.4114 - val_mean_squared_error: 59.4114\n",
      "Epoch 195/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 48.2000 - mean_squared_error: 48.2000 - val_loss: 60.0239 - val_mean_squared_error: 60.0239\n",
      "Epoch 196/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 48.9009 - mean_squared_error: 48.9009 - val_loss: 60.8747 - val_mean_squared_error: 60.8747\n",
      "Epoch 197/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 47.7350 - mean_squared_error: 47.7350 - val_loss: 60.5980 - val_mean_squared_error: 60.5980\n",
      "Epoch 198/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 47.4794 - mean_squared_error: 47.4794 - val_loss: 58.5590 - val_mean_squared_error: 58.5590\n",
      "Epoch 199/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 49.7263 - mean_squared_error: 49.7263 - val_loss: 55.0878 - val_mean_squared_error: 55.0878\n",
      "Epoch 200/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 48.9638 - mean_squared_error: 48.9638 - val_loss: 54.3168 - val_mean_squared_error: 54.3168\n",
      "Epoch 201/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 49.1080 - mean_squared_error: 49.1080 - val_loss: 55.6516 - val_mean_squared_error: 55.6516\n",
      "Epoch 202/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 44.7595 - mean_squared_error: 44.7595 - val_loss: 58.1396 - val_mean_squared_error: 58.1396\n",
      "Epoch 203/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 48.8557 - mean_squared_error: 48.8557 - val_loss: 58.3205 - val_mean_squared_error: 58.3205\n",
      "Epoch 204/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 47.3914 - mean_squared_error: 47.3914 - val_loss: 57.4160 - val_mean_squared_error: 57.4160\n",
      "Epoch 205/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 45.9809 - mean_squared_error: 45.9809 - val_loss: 55.6695 - val_mean_squared_error: 55.6695\n",
      "Epoch 206/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 48.6067 - mean_squared_error: 48.6067 - val_loss: 56.1931 - val_mean_squared_error: 56.1931\n",
      "Epoch 207/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 47.7774 - mean_squared_error: 47.7774 - val_loss: 56.4488 - val_mean_squared_error: 56.4488\n",
      "Epoch 208/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 43.1041 - mean_squared_error: 43.1041 - val_loss: 57.5423 - val_mean_squared_error: 57.5423\n",
      "Epoch 209/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 45.8783 - mean_squared_error: 45.8783 - val_loss: 57.1139 - val_mean_squared_error: 57.1139\n",
      "Epoch 210/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 44.7590 - mean_squared_error: 44.7590 - val_loss: 58.5521 - val_mean_squared_error: 58.5521\n",
      "Epoch 211/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 46.3647 - mean_squared_error: 46.3647 - val_loss: 57.9170 - val_mean_squared_error: 57.9170\n",
      "Epoch 212/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 49.7909 - mean_squared_error: 49.7909 - val_loss: 57.8357 - val_mean_squared_error: 57.8357\n",
      "Epoch 213/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 47.0230 - mean_squared_error: 47.0230 - val_loss: 56.7814 - val_mean_squared_error: 56.7814\n",
      "Epoch 214/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 46.4432 - mean_squared_error: 46.4432 - val_loss: 56.7662 - val_mean_squared_error: 56.7662\n",
      "Epoch 215/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 44.5024 - mean_squared_error: 44.5024 - val_loss: 57.4425 - val_mean_squared_error: 57.4425\n",
      "Epoch 216/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 46.1718 - mean_squared_error: 46.1718 - val_loss: 58.8022 - val_mean_squared_error: 58.8022\n",
      "Epoch 217/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 46.2142 - mean_squared_error: 46.2142 - val_loss: 58.4781 - val_mean_squared_error: 58.4781\n",
      "Epoch 218/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 45.1109 - mean_squared_error: 45.1109 - val_loss: 57.3054 - val_mean_squared_error: 57.3054\n",
      "Epoch 219/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 46.5911 - mean_squared_error: 46.5911 - val_loss: 57.3382 - val_mean_squared_error: 57.3382\n",
      "Epoch 220/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 46.8322 - mean_squared_error: 46.8322 - val_loss: 59.1314 - val_mean_squared_error: 59.1314\n",
      "Epoch 221/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 44.7729 - mean_squared_error: 44.7729 - val_loss: 59.6168 - val_mean_squared_error: 59.6168\n",
      "Epoch 222/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 44.0020 - mean_squared_error: 44.0020 - val_loss: 58.2167 - val_mean_squared_error: 58.2167\n",
      "Epoch 223/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 44.3640 - mean_squared_error: 44.3640 - val_loss: 57.7892 - val_mean_squared_error: 57.7892\n",
      "Epoch 224/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 44.9437 - mean_squared_error: 44.9437 - val_loss: 56.7139 - val_mean_squared_error: 56.7139\n",
      "Epoch 225/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 46.9336 - mean_squared_error: 46.9336 - val_loss: 60.1133 - val_mean_squared_error: 60.1133\n",
      "Epoch 226/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 46.7382 - mean_squared_error: 46.7382 - val_loss: 60.3115 - val_mean_squared_error: 60.3115\n",
      "Epoch 227/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 41.5538 - mean_squared_error: 41.5538 - val_loss: 58.5933 - val_mean_squared_error: 58.5933\n",
      "Epoch 228/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.8073 - mean_squared_error: 43.8073 - val_loss: 55.2351 - val_mean_squared_error: 55.2351\n",
      "Epoch 229/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 43.7471 - mean_squared_error: 43.7471 - val_loss: 56.0738 - val_mean_squared_error: 56.0738\n",
      "Epoch 230/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 50.0476 - mean_squared_error: 50.0476 - val_loss: 56.8901 - val_mean_squared_error: 56.8901\n",
      "Epoch 231/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 46.7083 - mean_squared_error: 46.7083 - val_loss: 56.2577 - val_mean_squared_error: 56.2577\n",
      "Epoch 232/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 47.0216 - mean_squared_error: 47.0216 - val_loss: 56.2712 - val_mean_squared_error: 56.2712\n",
      "Epoch 233/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 50.4185 - mean_squared_error: 50.4185 - val_loss: 55.3636 - val_mean_squared_error: 55.3636\n",
      "Epoch 234/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 43.0422 - mean_squared_error: 43.0422 - val_loss: 56.1923 - val_mean_squared_error: 56.1923\n",
      "Epoch 235/500\n",
      "216/216 [==============================] - ETA: 0s - loss: 60.4281 - mean_squared_error: 60.42 - 0s 44us/step - loss: 43.9105 - mean_squared_error: 43.9105 - val_loss: 57.6611 - val_mean_squared_error: 57.6611\n",
      "Epoch 236/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.4413 - mean_squared_error: 43.4413 - val_loss: 58.3043 - val_mean_squared_error: 58.3043\n",
      "Epoch 237/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 47.2813 - mean_squared_error: 47.2813 - val_loss: 56.5325 - val_mean_squared_error: 56.5325\n",
      "Epoch 238/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 45.2475 - mean_squared_error: 45.2475 - val_loss: 56.6167 - val_mean_squared_error: 56.6167\n",
      "Epoch 239/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 47.3579 - mean_squared_error: 47.3579 - val_loss: 57.5802 - val_mean_squared_error: 57.5802\n",
      "Epoch 240/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 46.3805 - mean_squared_error: 46.3805 - val_loss: 59.5095 - val_mean_squared_error: 59.5095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 45.1892 - mean_squared_error: 45.1892 - val_loss: 59.6859 - val_mean_squared_error: 59.6859\n",
      "Epoch 242/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 48.8107 - mean_squared_error: 48.8107 - val_loss: 57.5781 - val_mean_squared_error: 57.5781\n",
      "Epoch 243/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 45.1439 - mean_squared_error: 45.1439 - val_loss: 56.2859 - val_mean_squared_error: 56.2859\n",
      "Epoch 244/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 42.1481 - mean_squared_error: 42.1481 - val_loss: 57.7499 - val_mean_squared_error: 57.7499\n",
      "Epoch 245/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 44.7951 - mean_squared_error: 44.7951 - val_loss: 60.7382 - val_mean_squared_error: 60.7382\n",
      "Epoch 246/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 42.7120 - mean_squared_error: 42.7120 - val_loss: 62.2610 - val_mean_squared_error: 62.2610\n",
      "Epoch 247/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 44.9601 - mean_squared_error: 44.9601 - val_loss: 61.0285 - val_mean_squared_error: 61.0285\n",
      "Epoch 248/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 45.0578 - mean_squared_error: 45.0578 - val_loss: 58.5121 - val_mean_squared_error: 58.5121\n",
      "Epoch 249/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 44.2535 - mean_squared_error: 44.2535 - val_loss: 58.0023 - val_mean_squared_error: 58.0023\n",
      "Epoch 250/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.2237 - mean_squared_error: 43.2237 - val_loss: 56.8755 - val_mean_squared_error: 56.8755\n",
      "Epoch 251/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 44.2968 - mean_squared_error: 44.2968 - val_loss: 56.7078 - val_mean_squared_error: 56.7078\n",
      "Epoch 252/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 44.8801 - mean_squared_error: 44.8801 - val_loss: 56.9903 - val_mean_squared_error: 56.9903\n",
      "Epoch 253/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 44.9719 - mean_squared_error: 44.9719 - val_loss: 58.1332 - val_mean_squared_error: 58.1332\n",
      "Epoch 254/500\n",
      "216/216 [==============================] - 0s 54us/step - loss: 45.2658 - mean_squared_error: 45.2658 - val_loss: 58.8927 - val_mean_squared_error: 58.8927\n",
      "Epoch 255/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 44.0105 - mean_squared_error: 44.0105 - val_loss: 58.8635 - val_mean_squared_error: 58.8635\n",
      "Epoch 256/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 44.1190 - mean_squared_error: 44.1190 - val_loss: 58.1589 - val_mean_squared_error: 58.1589\n",
      "Epoch 257/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 43.9645 - mean_squared_error: 43.9645 - val_loss: 57.7111 - val_mean_squared_error: 57.7111\n",
      "Epoch 258/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.9563 - mean_squared_error: 43.9563 - val_loss: 55.2743 - val_mean_squared_error: 55.2743\n",
      "Epoch 259/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 40.4095 - mean_squared_error: 40.4095 - val_loss: 55.9737 - val_mean_squared_error: 55.9737\n",
      "Epoch 260/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 46.1855 - mean_squared_error: 46.1855 - val_loss: 55.1287 - val_mean_squared_error: 55.1287\n",
      "Epoch 261/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 43.0827 - mean_squared_error: 43.0827 - val_loss: 58.9682 - val_mean_squared_error: 58.9682\n",
      "Epoch 262/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 44.8098 - mean_squared_error: 44.8098 - val_loss: 59.4929 - val_mean_squared_error: 59.4929\n",
      "Epoch 263/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 45.2805 - mean_squared_error: 45.2805 - val_loss: 59.3889 - val_mean_squared_error: 59.3889\n",
      "Epoch 264/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 43.0223 - mean_squared_error: 43.0223 - val_loss: 56.5449 - val_mean_squared_error: 56.5449\n",
      "Epoch 265/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 45.6250 - mean_squared_error: 45.6250 - val_loss: 56.7863 - val_mean_squared_error: 56.7863\n",
      "Epoch 266/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 43.2094 - mean_squared_error: 43.2094 - val_loss: 57.2476 - val_mean_squared_error: 57.2476\n",
      "Epoch 267/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 40.7609 - mean_squared_error: 40.7609 - val_loss: 61.0955 - val_mean_squared_error: 61.0955\n",
      "Epoch 268/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 43.0903 - mean_squared_error: 43.0903 - val_loss: 60.3892 - val_mean_squared_error: 60.3892\n",
      "Epoch 269/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 45.1605 - mean_squared_error: 45.1605 - val_loss: 60.7425 - val_mean_squared_error: 60.7425\n",
      "Epoch 270/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 46.2829 - mean_squared_error: 46.2829 - val_loss: 57.7053 - val_mean_squared_error: 57.7053\n",
      "Epoch 271/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 42.5012 - mean_squared_error: 42.5012 - val_loss: 60.1941 - val_mean_squared_error: 60.1941\n",
      "Epoch 272/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 43.3529 - mean_squared_error: 43.3529 - val_loss: 60.3154 - val_mean_squared_error: 60.3154\n",
      "Epoch 273/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 44.8137 - mean_squared_error: 44.8137 - val_loss: 61.4074 - val_mean_squared_error: 61.4074\n",
      "Epoch 274/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 46.4091 - mean_squared_error: 46.4091 - val_loss: 61.6732 - val_mean_squared_error: 61.6732\n",
      "Epoch 275/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 40.6695 - mean_squared_error: 40.6695 - val_loss: 60.5613 - val_mean_squared_error: 60.5613\n",
      "Epoch 276/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.0342 - mean_squared_error: 43.0342 - val_loss: 59.3606 - val_mean_squared_error: 59.3606\n",
      "Epoch 277/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.1498 - mean_squared_error: 43.1498 - val_loss: 58.8506 - val_mean_squared_error: 58.8506\n",
      "Epoch 278/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.3334 - mean_squared_error: 43.3334 - val_loss: 59.0791 - val_mean_squared_error: 59.0791\n",
      "Epoch 279/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 40.5937 - mean_squared_error: 40.5937 - val_loss: 58.8975 - val_mean_squared_error: 58.8975\n",
      "Epoch 280/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.8514 - mean_squared_error: 42.8514 - val_loss: 58.6131 - val_mean_squared_error: 58.6131\n",
      "Epoch 281/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 44.2130 - mean_squared_error: 44.2130 - val_loss: 60.7397 - val_mean_squared_error: 60.7397\n",
      "Epoch 282/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 45.2564 - mean_squared_error: 45.2564 - val_loss: 57.5026 - val_mean_squared_error: 57.5026\n",
      "Epoch 283/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 41.7485 - mean_squared_error: 41.7485 - val_loss: 55.7960 - val_mean_squared_error: 55.7960\n",
      "Epoch 284/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 41.8814 - mean_squared_error: 41.8814 - val_loss: 57.2619 - val_mean_squared_error: 57.2619\n",
      "Epoch 285/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 40.8451 - mean_squared_error: 40.8451 - val_loss: 57.7536 - val_mean_squared_error: 57.7536\n",
      "Epoch 286/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 44.0301 - mean_squared_error: 44.0301 - val_loss: 58.5644 - val_mean_squared_error: 58.5644\n",
      "Epoch 287/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.6381 - mean_squared_error: 42.6381 - val_loss: 57.8825 - val_mean_squared_error: 57.8825\n",
      "Epoch 288/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.2555 - mean_squared_error: 42.2555 - val_loss: 56.7800 - val_mean_squared_error: 56.7800\n",
      "Epoch 289/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 43us/step - loss: 41.0618 - mean_squared_error: 41.0618 - val_loss: 57.0625 - val_mean_squared_error: 57.0625\n",
      "Epoch 290/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 43.1478 - mean_squared_error: 43.1478 - val_loss: 57.1546 - val_mean_squared_error: 57.1546\n",
      "Epoch 291/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 42.6075 - mean_squared_error: 42.6075 - val_loss: 59.9319 - val_mean_squared_error: 59.9319\n",
      "Epoch 292/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 43.7243 - mean_squared_error: 43.7243 - val_loss: 58.0576 - val_mean_squared_error: 58.0576\n",
      "Epoch 293/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 45.2513 - mean_squared_error: 45.2513 - val_loss: 53.3215 - val_mean_squared_error: 53.3215\n",
      "Epoch 294/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 46.6156 - mean_squared_error: 46.6156 - val_loss: 53.5698 - val_mean_squared_error: 53.5698\n",
      "Epoch 295/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 40.6923 - mean_squared_error: 40.6923 - val_loss: 59.5324 - val_mean_squared_error: 59.5324\n",
      "Epoch 296/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 41.1234 - mean_squared_error: 41.1234 - val_loss: 61.8901 - val_mean_squared_error: 61.8901\n",
      "Epoch 297/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 42.9649 - mean_squared_error: 42.9649 - val_loss: 59.2309 - val_mean_squared_error: 59.2309\n",
      "Epoch 298/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 42.5659 - mean_squared_error: 42.5659 - val_loss: 57.5782 - val_mean_squared_error: 57.5782\n",
      "Epoch 299/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.0291 - mean_squared_error: 43.0291 - val_loss: 59.1616 - val_mean_squared_error: 59.1616\n",
      "Epoch 300/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 40.6486 - mean_squared_error: 40.6486 - val_loss: 61.1818 - val_mean_squared_error: 61.1818\n",
      "Epoch 301/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.2499 - mean_squared_error: 43.2499 - val_loss: 59.9352 - val_mean_squared_error: 59.9352\n",
      "Epoch 302/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 40.0188 - mean_squared_error: 40.0188 - val_loss: 56.5156 - val_mean_squared_error: 56.5156\n",
      "Epoch 303/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.4890 - mean_squared_error: 42.4890 - val_loss: 55.4025 - val_mean_squared_error: 55.4025\n",
      "Epoch 304/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.4242 - mean_squared_error: 43.4242 - val_loss: 56.3904 - val_mean_squared_error: 56.3904\n",
      "Epoch 305/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 43.1731 - mean_squared_error: 43.1731 - val_loss: 58.2229 - val_mean_squared_error: 58.2229\n",
      "Epoch 306/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 39.9656 - mean_squared_error: 39.9656 - val_loss: 60.5317 - val_mean_squared_error: 60.5317\n",
      "Epoch 307/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 42.8910 - mean_squared_error: 42.8910 - val_loss: 60.2688 - val_mean_squared_error: 60.2688\n",
      "Epoch 308/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 42.3609 - mean_squared_error: 42.3609 - val_loss: 59.5147 - val_mean_squared_error: 59.5147\n",
      "Epoch 309/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 43.4975 - mean_squared_error: 43.4975 - val_loss: 58.2706 - val_mean_squared_error: 58.2706\n",
      "Epoch 310/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 45.4782 - mean_squared_error: 45.4782 - val_loss: 57.7665 - val_mean_squared_error: 57.7665\n",
      "Epoch 311/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 44.8176 - mean_squared_error: 44.8176 - val_loss: 56.8693 - val_mean_squared_error: 56.8693\n",
      "Epoch 312/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 43.3959 - mean_squared_error: 43.3959 - val_loss: 61.1624 - val_mean_squared_error: 61.1624\n",
      "Epoch 313/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 46.3216 - mean_squared_error: 46.3216 - val_loss: 59.4276 - val_mean_squared_error: 59.4276\n",
      "Epoch 314/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 44.2293 - mean_squared_error: 44.2293 - val_loss: 57.6361 - val_mean_squared_error: 57.6361\n",
      "Epoch 315/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 41.6334 - mean_squared_error: 41.6334 - val_loss: 58.3883 - val_mean_squared_error: 58.3883\n",
      "Epoch 316/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 39.9672 - mean_squared_error: 39.9672 - val_loss: 59.9804 - val_mean_squared_error: 59.9804\n",
      "Epoch 317/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 41.2000 - mean_squared_error: 41.2000 - val_loss: 60.9308 - val_mean_squared_error: 60.9308\n",
      "Epoch 318/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 42.1218 - mean_squared_error: 42.1218 - val_loss: 59.1092 - val_mean_squared_error: 59.1092\n",
      "Epoch 319/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.5541 - mean_squared_error: 42.5541 - val_loss: 60.4093 - val_mean_squared_error: 60.4093\n",
      "Epoch 320/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.5669 - mean_squared_error: 43.5669 - val_loss: 56.7395 - val_mean_squared_error: 56.7395\n",
      "Epoch 321/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 42.4790 - mean_squared_error: 42.4790 - val_loss: 56.5171 - val_mean_squared_error: 56.5171\n",
      "Epoch 322/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 42.7360 - mean_squared_error: 42.7360 - val_loss: 58.5662 - val_mean_squared_error: 58.5662\n",
      "Epoch 323/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 40.2715 - mean_squared_error: 40.2715 - val_loss: 58.0523 - val_mean_squared_error: 58.0523\n",
      "Epoch 324/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 43.8120 - mean_squared_error: 43.8120 - val_loss: 56.9983 - val_mean_squared_error: 56.9983\n",
      "Epoch 325/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 38.0184 - mean_squared_error: 38.0184 - val_loss: 56.5595 - val_mean_squared_error: 56.5595\n",
      "Epoch 326/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 43.9270 - mean_squared_error: 43.9270 - val_loss: 57.6680 - val_mean_squared_error: 57.6680\n",
      "Epoch 327/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 40.5643 - mean_squared_error: 40.5643 - val_loss: 61.4932 - val_mean_squared_error: 61.4932\n",
      "Epoch 328/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 42.9439 - mean_squared_error: 42.9439 - val_loss: 60.2560 - val_mean_squared_error: 60.2560\n",
      "Epoch 329/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 41.1783 - mean_squared_error: 41.1783 - val_loss: 59.9226 - val_mean_squared_error: 59.9226\n",
      "Epoch 330/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 43.6008 - mean_squared_error: 43.6008 - val_loss: 59.5684 - val_mean_squared_error: 59.5684\n",
      "Epoch 331/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 42.3361 - mean_squared_error: 42.3361 - val_loss: 61.2212 - val_mean_squared_error: 61.2212\n",
      "Epoch 332/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 38.8605 - mean_squared_error: 38.8605 - val_loss: 61.1022 - val_mean_squared_error: 61.1022\n",
      "Epoch 333/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 44.2694 - mean_squared_error: 44.2694 - val_loss: 59.2874 - val_mean_squared_error: 59.2874\n",
      "Epoch 334/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 42.5593 - mean_squared_error: 42.5593 - val_loss: 57.7973 - val_mean_squared_error: 57.7973\n",
      "Epoch 335/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 42.3702 - mean_squared_error: 42.3702 - val_loss: 56.9751 - val_mean_squared_error: 56.9751\n",
      "Epoch 336/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 41.1672 - mean_squared_error: 41.1672 - val_loss: 57.1046 - val_mean_squared_error: 57.1046\n",
      "Epoch 337/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 46us/step - loss: 41.5494 - mean_squared_error: 41.5494 - val_loss: 57.3936 - val_mean_squared_error: 57.3936\n",
      "Epoch 338/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 39.6427 - mean_squared_error: 39.6427 - val_loss: 57.9042 - val_mean_squared_error: 57.9042\n",
      "Epoch 339/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 39.5050 - mean_squared_error: 39.5050 - val_loss: 58.3837 - val_mean_squared_error: 58.3837\n",
      "Epoch 340/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 43.8567 - mean_squared_error: 43.8567 - val_loss: 57.1094 - val_mean_squared_error: 57.1094\n",
      "Epoch 341/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 44.0671 - mean_squared_error: 44.0671 - val_loss: 55.8517 - val_mean_squared_error: 55.8517\n",
      "Epoch 342/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 37.9042 - mean_squared_error: 37.9042 - val_loss: 56.3006 - val_mean_squared_error: 56.3006\n",
      "Epoch 343/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 42.3768 - mean_squared_error: 42.3768 - val_loss: 55.0792 - val_mean_squared_error: 55.0792\n",
      "Epoch 344/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 42.2149 - mean_squared_error: 42.2149 - val_loss: 56.4344 - val_mean_squared_error: 56.4344\n",
      "Epoch 345/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 42.6234 - mean_squared_error: 42.6234 - val_loss: 56.1160 - val_mean_squared_error: 56.1160\n",
      "Epoch 346/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 40.5311 - mean_squared_error: 40.5311 - val_loss: 62.7272 - val_mean_squared_error: 62.7272\n",
      "Epoch 347/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 44.4356 - mean_squared_error: 44.4356 - val_loss: 60.1845 - val_mean_squared_error: 60.1845\n",
      "Epoch 348/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.1344 - mean_squared_error: 43.1344 - val_loss: 61.4558 - val_mean_squared_error: 61.4558\n",
      "Epoch 349/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 41.2576 - mean_squared_error: 41.2576 - val_loss: 62.9072 - val_mean_squared_error: 62.9072\n",
      "Epoch 350/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 42.6481 - mean_squared_error: 42.6481 - val_loss: 65.1543 - val_mean_squared_error: 65.1543\n",
      "Epoch 351/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 42.5594 - mean_squared_error: 42.5594 - val_loss: 59.8896 - val_mean_squared_error: 59.8896\n",
      "Epoch 352/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 42.1141 - mean_squared_error: 42.1141 - val_loss: 57.7953 - val_mean_squared_error: 57.7953\n",
      "Epoch 353/500\n",
      "216/216 [==============================] - 0s 55us/step - loss: 42.4681 - mean_squared_error: 42.4681 - val_loss: 58.2014 - val_mean_squared_error: 58.2014\n",
      "Epoch 354/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 40.9790 - mean_squared_error: 40.9790 - val_loss: 60.1875 - val_mean_squared_error: 60.1875\n",
      "Epoch 355/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 39.0889 - mean_squared_error: 39.0889 - val_loss: 61.3619 - val_mean_squared_error: 61.3619\n",
      "Epoch 356/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 40.7261 - mean_squared_error: 40.7261 - val_loss: 63.9627 - val_mean_squared_error: 63.9627\n",
      "Epoch 357/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 43.1849 - mean_squared_error: 43.1849 - val_loss: 61.2516 - val_mean_squared_error: 61.2516\n",
      "Epoch 358/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 40.1092 - mean_squared_error: 40.1092 - val_loss: 64.2397 - val_mean_squared_error: 64.2397\n",
      "Epoch 359/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 41.6774 - mean_squared_error: 41.6774 - val_loss: 59.9806 - val_mean_squared_error: 59.9806\n",
      "Epoch 360/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 40.8140 - mean_squared_error: 40.8140 - val_loss: 62.7512 - val_mean_squared_error: 62.7512\n",
      "Epoch 361/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 43.5525 - mean_squared_error: 43.5525 - val_loss: 62.5382 - val_mean_squared_error: 62.5382\n",
      "Epoch 362/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 37.1965 - mean_squared_error: 37.1965 - val_loss: 58.5257 - val_mean_squared_error: 58.5257\n",
      "Epoch 363/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 43.0367 - mean_squared_error: 43.0367 - val_loss: 54.7631 - val_mean_squared_error: 54.7631\n",
      "Epoch 364/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 39.3240 - mean_squared_error: 39.3240 - val_loss: 54.4599 - val_mean_squared_error: 54.4599\n",
      "Epoch 365/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 41.3778 - mean_squared_error: 41.3778 - val_loss: 54.9815 - val_mean_squared_error: 54.9815\n",
      "Epoch 366/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 37.4642 - mean_squared_error: 37.4642 - val_loss: 58.5823 - val_mean_squared_error: 58.5823\n",
      "Epoch 367/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 41.5547 - mean_squared_error: 41.5547 - val_loss: 60.0239 - val_mean_squared_error: 60.0239\n",
      "Epoch 368/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 45.7675 - mean_squared_error: 45.7675 - val_loss: 58.3743 - val_mean_squared_error: 58.3743\n",
      "Epoch 369/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 39.1963 - mean_squared_error: 39.1963 - val_loss: 57.3815 - val_mean_squared_error: 57.3815\n",
      "Epoch 370/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 39.5996 - mean_squared_error: 39.5996 - val_loss: 61.3092 - val_mean_squared_error: 61.3092\n",
      "Epoch 371/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 39.2067 - mean_squared_error: 39.2067 - val_loss: 60.6876 - val_mean_squared_error: 60.6876\n",
      "Epoch 372/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 40.3318 - mean_squared_error: 40.3318 - val_loss: 59.8981 - val_mean_squared_error: 59.8981\n",
      "Epoch 373/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 40.5651 - mean_squared_error: 40.5651 - val_loss: 59.3688 - val_mean_squared_error: 59.3688\n",
      "Epoch 374/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 43.1086 - mean_squared_error: 43.1086 - val_loss: 58.5735 - val_mean_squared_error: 58.5735\n",
      "Epoch 375/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 39.1972 - mean_squared_error: 39.1972 - val_loss: 59.6956 - val_mean_squared_error: 59.6956\n",
      "Epoch 376/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 41.4453 - mean_squared_error: 41.4453 - val_loss: 60.1664 - val_mean_squared_error: 60.1664\n",
      "Epoch 377/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 37.6958 - mean_squared_error: 37.6958 - val_loss: 60.0811 - val_mean_squared_error: 60.0811\n",
      "Epoch 378/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 40.3329 - mean_squared_error: 40.3329 - val_loss: 56.9695 - val_mean_squared_error: 56.9695\n",
      "Epoch 379/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 40.3854 - mean_squared_error: 40.3854 - val_loss: 56.2847 - val_mean_squared_error: 56.2847\n",
      "Epoch 380/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 42.0790 - mean_squared_error: 42.0790 - val_loss: 57.5530 - val_mean_squared_error: 57.5530\n",
      "Epoch 381/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 38.3970 - mean_squared_error: 38.3970 - val_loss: 61.3281 - val_mean_squared_error: 61.3281\n",
      "Epoch 382/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 37.6781 - mean_squared_error: 37.6781 - val_loss: 58.0264 - val_mean_squared_error: 58.0264\n",
      "Epoch 383/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 39.9601 - mean_squared_error: 39.9601 - val_loss: 56.9755 - val_mean_squared_error: 56.9755\n",
      "Epoch 384/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 42.3126 - mean_squared_error: 42.3126 - val_loss: 59.5759 - val_mean_squared_error: 59.5759\n",
      "Epoch 385/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 49us/step - loss: 41.2096 - mean_squared_error: 41.2096 - val_loss: 62.1689 - val_mean_squared_error: 62.1689\n",
      "Epoch 386/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 38.7039 - mean_squared_error: 38.7039 - val_loss: 59.2236 - val_mean_squared_error: 59.2236\n",
      "Epoch 387/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 37.8369 - mean_squared_error: 37.8369 - val_loss: 58.0822 - val_mean_squared_error: 58.0822\n",
      "Epoch 388/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 39.2566 - mean_squared_error: 39.2566 - val_loss: 58.0209 - val_mean_squared_error: 58.0209\n",
      "Epoch 389/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 42.0522 - mean_squared_error: 42.0522 - val_loss: 57.8958 - val_mean_squared_error: 57.8958\n",
      "Epoch 390/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 40.4719 - mean_squared_error: 40.4719 - val_loss: 57.8870 - val_mean_squared_error: 57.8870\n",
      "Epoch 391/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 39.1027 - mean_squared_error: 39.1027 - val_loss: 59.2308 - val_mean_squared_error: 59.2308\n",
      "Epoch 392/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 40.5620 - mean_squared_error: 40.5620 - val_loss: 59.3747 - val_mean_squared_error: 59.3747\n",
      "Epoch 393/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 38.0705 - mean_squared_error: 38.0705 - val_loss: 57.1393 - val_mean_squared_error: 57.1393\n",
      "Epoch 394/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 35.6291 - mean_squared_error: 35.6291 - val_loss: 57.4974 - val_mean_squared_error: 57.4974\n",
      "Epoch 395/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 35.7751 - mean_squared_error: 35.7751 - val_loss: 57.8720 - val_mean_squared_error: 57.8720\n",
      "Epoch 396/500\n",
      "216/216 [==============================] - 0s 55us/step - loss: 36.9386 - mean_squared_error: 36.9386 - val_loss: 59.1780 - val_mean_squared_error: 59.1780\n",
      "Epoch 397/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 38.9307 - mean_squared_error: 38.9307 - val_loss: 59.4373 - val_mean_squared_error: 59.4373\n",
      "Epoch 398/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 36.0926 - mean_squared_error: 36.0926 - val_loss: 61.0135 - val_mean_squared_error: 61.0135\n",
      "Epoch 399/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 39.5711 - mean_squared_error: 39.5711 - val_loss: 61.3374 - val_mean_squared_error: 61.3374\n",
      "Epoch 400/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 37.3102 - mean_squared_error: 37.3102 - val_loss: 57.6587 - val_mean_squared_error: 57.6587\n",
      "Epoch 401/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 35.2141 - mean_squared_error: 35.2141 - val_loss: 54.3085 - val_mean_squared_error: 54.3085\n",
      "Epoch 402/500\n",
      "216/216 [==============================] - 0s 56us/step - loss: 37.2315 - mean_squared_error: 37.2315 - val_loss: 54.7955 - val_mean_squared_error: 54.7955\n",
      "Epoch 403/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 42.6338 - mean_squared_error: 42.6338 - val_loss: 57.0153 - val_mean_squared_error: 57.0153\n",
      "Epoch 404/500\n",
      "216/216 [==============================] - 0s 56us/step - loss: 38.8228 - mean_squared_error: 38.8228 - val_loss: 60.4774 - val_mean_squared_error: 60.4774\n",
      "Epoch 405/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 37.7519 - mean_squared_error: 37.7519 - val_loss: 60.7411 - val_mean_squared_error: 60.7411\n",
      "Epoch 406/500\n",
      "216/216 [==============================] - 0s 58us/step - loss: 36.9682 - mean_squared_error: 36.9682 - val_loss: 59.0131 - val_mean_squared_error: 59.0131\n",
      "Epoch 407/500\n",
      "216/216 [==============================] - 0s 54us/step - loss: 40.2540 - mean_squared_error: 40.2540 - val_loss: 57.1946 - val_mean_squared_error: 57.1946\n",
      "Epoch 408/500\n",
      "216/216 [==============================] - 0s 53us/step - loss: 41.5835 - mean_squared_error: 41.5835 - val_loss: 54.9435 - val_mean_squared_error: 54.9435\n",
      "Epoch 409/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 38.8444 - mean_squared_error: 38.8444 - val_loss: 55.1564 - val_mean_squared_error: 55.1564\n",
      "Epoch 410/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 37.7754 - mean_squared_error: 37.7754 - val_loss: 54.7444 - val_mean_squared_error: 54.7444\n",
      "Epoch 411/500\n",
      "216/216 [==============================] - 0s 60us/step - loss: 39.8234 - mean_squared_error: 39.8234 - val_loss: 52.1811 - val_mean_squared_error: 52.1811\n",
      "Epoch 412/500\n",
      "216/216 [==============================] - 0s 54us/step - loss: 38.9028 - mean_squared_error: 38.9028 - val_loss: 51.6588 - val_mean_squared_error: 51.6588\n",
      "Epoch 413/500\n",
      "216/216 [==============================] - 0s 58us/step - loss: 36.2461 - mean_squared_error: 36.2461 - val_loss: 52.6663 - val_mean_squared_error: 52.6663\n",
      "Epoch 414/500\n",
      "216/216 [==============================] - 0s 56us/step - loss: 37.3781 - mean_squared_error: 37.3781 - val_loss: 53.4272 - val_mean_squared_error: 53.4272\n",
      "Epoch 415/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 38.9552 - mean_squared_error: 38.9552 - val_loss: 59.5948 - val_mean_squared_error: 59.5948\n",
      "Epoch 416/500\n",
      "216/216 [==============================] - 0s 59us/step - loss: 39.8495 - mean_squared_error: 39.8495 - val_loss: 61.7825 - val_mean_squared_error: 61.7825\n",
      "Epoch 417/500\n",
      "216/216 [==============================] - 0s 56us/step - loss: 38.5409 - mean_squared_error: 38.5409 - val_loss: 62.1033 - val_mean_squared_error: 62.1033\n",
      "Epoch 418/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 39.7359 - mean_squared_error: 39.7359 - val_loss: 61.7408 - val_mean_squared_error: 61.7408\n",
      "Epoch 419/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 40.7659 - mean_squared_error: 40.7659 - val_loss: 61.5925 - val_mean_squared_error: 61.5925\n",
      "Epoch 420/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 37.1129 - mean_squared_error: 37.1129 - val_loss: 56.6451 - val_mean_squared_error: 56.6451\n",
      "Epoch 421/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 36.4184 - mean_squared_error: 36.4184 - val_loss: 55.9089 - val_mean_squared_error: 55.9089\n",
      "Epoch 422/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 36.1551 - mean_squared_error: 36.1551 - val_loss: 57.5531 - val_mean_squared_error: 57.5531\n",
      "Epoch 423/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 38.7150 - mean_squared_error: 38.7150 - val_loss: 58.3075 - val_mean_squared_error: 58.3075\n",
      "Epoch 424/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 37.5854 - mean_squared_error: 37.5854 - val_loss: 57.5676 - val_mean_squared_error: 57.5676\n",
      "Epoch 425/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 36.6746 - mean_squared_error: 36.6746 - val_loss: 58.3834 - val_mean_squared_error: 58.3834\n",
      "Epoch 426/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 35.5734 - mean_squared_error: 35.5734 - val_loss: 63.2788 - val_mean_squared_error: 63.2788\n",
      "Epoch 427/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 41.5558 - mean_squared_error: 41.5558 - val_loss: 61.7270 - val_mean_squared_error: 61.7270\n",
      "Epoch 428/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 40.3458 - mean_squared_error: 40.3458 - val_loss: 60.2522 - val_mean_squared_error: 60.2522\n",
      "Epoch 429/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 38.9003 - mean_squared_error: 38.9003 - val_loss: 58.7264 - val_mean_squared_error: 58.7264\n",
      "Epoch 430/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 36.2502 - mean_squared_error: 36.2502 - val_loss: 58.9233 - val_mean_squared_error: 58.9233\n",
      "Epoch 431/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 41.4188 - mean_squared_error: 41.4188 - val_loss: 56.9682 - val_mean_squared_error: 56.9682\n",
      "Epoch 432/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 36.1796 - mean_squared_error: 36.1796 - val_loss: 57.9250 - val_mean_squared_error: 57.9250\n",
      "Epoch 433/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 47us/step - loss: 37.1834 - mean_squared_error: 37.1834 - val_loss: 60.6066 - val_mean_squared_error: 60.6066\n",
      "Epoch 434/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 36.0557 - mean_squared_error: 36.0557 - val_loss: 61.4135 - val_mean_squared_error: 61.4135\n",
      "Epoch 435/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 36.8707 - mean_squared_error: 36.8707 - val_loss: 57.9232 - val_mean_squared_error: 57.9232\n",
      "Epoch 436/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 38.4445 - mean_squared_error: 38.4445 - val_loss: 57.4422 - val_mean_squared_error: 57.4422\n",
      "Epoch 437/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 37.2846 - mean_squared_error: 37.2846 - val_loss: 56.5193 - val_mean_squared_error: 56.5193\n",
      "Epoch 438/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 38.5479 - mean_squared_error: 38.5479 - val_loss: 59.9209 - val_mean_squared_error: 59.9209\n",
      "Epoch 439/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 39.8289 - mean_squared_error: 39.8289 - val_loss: 60.6868 - val_mean_squared_error: 60.6868\n",
      "Epoch 440/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 38.0704 - mean_squared_error: 38.0704 - val_loss: 62.9117 - val_mean_squared_error: 62.9117\n",
      "Epoch 441/500\n",
      "216/216 [==============================] - 0s 52us/step - loss: 36.0103 - mean_squared_error: 36.0103 - val_loss: 62.1871 - val_mean_squared_error: 62.1871\n",
      "Epoch 442/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 39.1084 - mean_squared_error: 39.1084 - val_loss: 61.3533 - val_mean_squared_error: 61.3533\n",
      "Epoch 443/500\n",
      "216/216 [==============================] - 0s 38us/step - loss: 33.8907 - mean_squared_error: 33.8907 - val_loss: 57.6075 - val_mean_squared_error: 57.6075\n",
      "Epoch 444/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 34.5760 - mean_squared_error: 34.5760 - val_loss: 58.4835 - val_mean_squared_error: 58.4835\n",
      "Epoch 445/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 40.4200 - mean_squared_error: 40.4200 - val_loss: 58.9520 - val_mean_squared_error: 58.9520\n",
      "Epoch 446/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 37.1182 - mean_squared_error: 37.1182 - val_loss: 57.8259 - val_mean_squared_error: 57.8259\n",
      "Epoch 447/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 37.0717 - mean_squared_error: 37.0717 - val_loss: 55.7747 - val_mean_squared_error: 55.7747\n",
      "Epoch 448/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 39.4991 - mean_squared_error: 39.4991 - val_loss: 55.4988 - val_mean_squared_error: 55.4988\n",
      "Epoch 449/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 37.3960 - mean_squared_error: 37.3960 - val_loss: 55.4854 - val_mean_squared_error: 55.4854\n",
      "Epoch 450/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 35.5769 - mean_squared_error: 35.5769 - val_loss: 55.7293 - val_mean_squared_error: 55.7293\n",
      "Epoch 451/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 37.1267 - mean_squared_error: 37.1267 - val_loss: 56.9247 - val_mean_squared_error: 56.9247\n",
      "Epoch 452/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 38.4677 - mean_squared_error: 38.4677 - val_loss: 60.3590 - val_mean_squared_error: 60.3590\n",
      "Epoch 453/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 37.0384 - mean_squared_error: 37.0384 - val_loss: 59.1276 - val_mean_squared_error: 59.1276\n",
      "Epoch 454/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 35.6071 - mean_squared_error: 35.6071 - val_loss: 56.8551 - val_mean_squared_error: 56.8551\n",
      "Epoch 455/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 35.5702 - mean_squared_error: 35.5702 - val_loss: 56.9005 - val_mean_squared_error: 56.9005\n",
      "Epoch 456/500\n",
      "216/216 [==============================] - 0s 39us/step - loss: 34.3188 - mean_squared_error: 34.3188 - val_loss: 60.1941 - val_mean_squared_error: 60.1941\n",
      "Epoch 457/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 38.8097 - mean_squared_error: 38.8097 - val_loss: 61.5786 - val_mean_squared_error: 61.5786\n",
      "Epoch 458/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 36.6711 - mean_squared_error: 36.6711 - val_loss: 64.0318 - val_mean_squared_error: 64.0318\n",
      "Epoch 459/500\n",
      "216/216 [==============================] - 0s 50us/step - loss: 32.9095 - mean_squared_error: 32.9095 - val_loss: 63.3988 - val_mean_squared_error: 63.3988\n",
      "Epoch 460/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 33.2477 - mean_squared_error: 33.2477 - val_loss: 60.8499 - val_mean_squared_error: 60.8499\n",
      "Epoch 461/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 34.0648 - mean_squared_error: 34.0648 - val_loss: 56.9431 - val_mean_squared_error: 56.9431\n",
      "Epoch 462/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 37.6888 - mean_squared_error: 37.6888 - val_loss: 58.6576 - val_mean_squared_error: 58.6576\n",
      "Epoch 463/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 37.4468 - mean_squared_error: 37.4468 - val_loss: 62.1454 - val_mean_squared_error: 62.1454\n",
      "Epoch 464/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 38.2369 - mean_squared_error: 38.2369 - val_loss: 61.1069 - val_mean_squared_error: 61.1069\n",
      "Epoch 465/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 34.9553 - mean_squared_error: 34.9553 - val_loss: 59.8761 - val_mean_squared_error: 59.8761\n",
      "Epoch 466/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 37.1232 - mean_squared_error: 37.1232 - val_loss: 58.1316 - val_mean_squared_error: 58.1316\n",
      "Epoch 467/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 37.9314 - mean_squared_error: 37.9314 - val_loss: 59.6477 - val_mean_squared_error: 59.6477\n",
      "Epoch 468/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 37.5982 - mean_squared_error: 37.5982 - val_loss: 61.9019 - val_mean_squared_error: 61.9019\n",
      "Epoch 469/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 37.7191 - mean_squared_error: 37.7191 - val_loss: 60.1527 - val_mean_squared_error: 60.1527\n",
      "Epoch 470/500\n",
      "216/216 [==============================] - 0s 40us/step - loss: 36.9586 - mean_squared_error: 36.9586 - val_loss: 58.3738 - val_mean_squared_error: 58.3738\n",
      "Epoch 471/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 34.8361 - mean_squared_error: 34.8361 - val_loss: 56.2770 - val_mean_squared_error: 56.2770\n",
      "Epoch 472/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 34.0568 - mean_squared_error: 34.0568 - val_loss: 54.2221 - val_mean_squared_error: 54.2221\n",
      "Epoch 473/500\n",
      "216/216 [==============================] - 0s 41us/step - loss: 38.9707 - mean_squared_error: 38.9707 - val_loss: 56.2303 - val_mean_squared_error: 56.2303\n",
      "Epoch 474/500\n",
      "216/216 [==============================] - 0s 45us/step - loss: 34.5649 - mean_squared_error: 34.5649 - val_loss: 58.3126 - val_mean_squared_error: 58.3126\n",
      "Epoch 475/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 32.6874 - mean_squared_error: 32.6874 - val_loss: 60.7141 - val_mean_squared_error: 60.7141\n",
      "Epoch 476/500\n",
      "216/216 [==============================] - 0s 37us/step - loss: 34.4447 - mean_squared_error: 34.4447 - val_loss: 60.7311 - val_mean_squared_error: 60.7311\n",
      "Epoch 477/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 34.7918 - mean_squared_error: 34.7918 - val_loss: 58.1102 - val_mean_squared_error: 58.1102\n",
      "Epoch 478/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 32.8684 - mean_squared_error: 32.8684 - val_loss: 58.5161 - val_mean_squared_error: 58.5161\n",
      "Epoch 479/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 33.8755 - mean_squared_error: 33.8755 - val_loss: 57.6957 - val_mean_squared_error: 57.6957\n",
      "Epoch 480/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 36.0964 - mean_squared_error: 36.0964 - val_loss: 56.5570 - val_mean_squared_error: 56.5570\n",
      "Epoch 481/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 0s 41us/step - loss: 38.0815 - mean_squared_error: 38.0815 - val_loss: 56.2555 - val_mean_squared_error: 56.2555\n",
      "Epoch 482/500\n",
      "216/216 [==============================] - 0s 43us/step - loss: 32.3110 - mean_squared_error: 32.3110 - val_loss: 58.9156 - val_mean_squared_error: 58.9156\n",
      "Epoch 483/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 32.3950 - mean_squared_error: 32.3950 - val_loss: 59.0413 - val_mean_squared_error: 59.0413\n",
      "Epoch 484/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 33.8841 - mean_squared_error: 33.8841 - val_loss: 60.3588 - val_mean_squared_error: 60.3588\n",
      "Epoch 485/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 36.7720 - mean_squared_error: 36.7720 - val_loss: 62.9113 - val_mean_squared_error: 62.9113\n",
      "Epoch 486/500\n",
      "216/216 [==============================] - 0s 42us/step - loss: 36.8873 - mean_squared_error: 36.8873 - val_loss: 58.9657 - val_mean_squared_error: 58.9657\n",
      "Epoch 487/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 34.9769 - mean_squared_error: 34.9769 - val_loss: 57.1532 - val_mean_squared_error: 57.1532\n",
      "Epoch 488/500\n",
      "216/216 [==============================] - 0s 48us/step - loss: 31.4585 - mean_squared_error: 31.4585 - val_loss: 55.9528 - val_mean_squared_error: 55.9528\n",
      "Epoch 489/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 36.8799 - mean_squared_error: 36.8799 - val_loss: 57.3834 - val_mean_squared_error: 57.3834\n",
      "Epoch 490/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 35.1220 - mean_squared_error: 35.1220 - val_loss: 62.5493 - val_mean_squared_error: 62.5493\n",
      "Epoch 491/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 38.6503 - mean_squared_error: 38.6503 - val_loss: 59.9219 - val_mean_squared_error: 59.9219\n",
      "Epoch 492/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 35.3515 - mean_squared_error: 35.3515 - val_loss: 56.4009 - val_mean_squared_error: 56.4009\n",
      "Epoch 493/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 33.7634 - mean_squared_error: 33.7634 - val_loss: 55.1544 - val_mean_squared_error: 55.1544\n",
      "Epoch 494/500\n",
      "216/216 [==============================] - 0s 51us/step - loss: 31.8450 - mean_squared_error: 31.8450 - val_loss: 54.3744 - val_mean_squared_error: 54.3744\n",
      "Epoch 495/500\n",
      "216/216 [==============================] - 0s 54us/step - loss: 35.0273 - mean_squared_error: 35.0273 - val_loss: 54.1641 - val_mean_squared_error: 54.1641\n",
      "Epoch 496/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 34.8093 - mean_squared_error: 34.8093 - val_loss: 56.7918 - val_mean_squared_error: 56.7918\n",
      "Epoch 497/500\n",
      "216/216 [==============================] - 0s 46us/step - loss: 33.2341 - mean_squared_error: 33.2341 - val_loss: 58.1866 - val_mean_squared_error: 58.1866\n",
      "Epoch 498/500\n",
      "216/216 [==============================] - 0s 47us/step - loss: 33.3082 - mean_squared_error: 33.3082 - val_loss: 59.6115 - val_mean_squared_error: 59.6115\n",
      "Epoch 499/500\n",
      "216/216 [==============================] - 0s 44us/step - loss: 34.0529 - mean_squared_error: 34.0529 - val_loss: 56.6499 - val_mean_squared_error: 56.6499\n",
      "Epoch 500/500\n",
      "216/216 [==============================] - 0s 49us/step - loss: 33.0107 - mean_squared_error: 33.0107 - val_loss: 55.2284 - val_mean_squared_error: 55.2284\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    " batch_size=128,\n",
    " epochs=500,\n",
    " verbose=1,\n",
    " validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 70.23565419514973\n",
      "Accuracy: 70.23565419514973\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Loss:', score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
